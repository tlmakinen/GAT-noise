{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gat-noise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpXa2IK0WH5z",
        "colab_type": "code",
        "outputId": "edd63569-8909-4b4e-be2b-9ca2a3e67398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/Diego999/pyGAT.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyGAT'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Total 107 (delta 0), reused 0 (delta 0), pack-reused 107\u001b[K\n",
            "Receiving objects: 100% (107/107), 202.66 KiB | 2.23 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN_XiIiRJnqc",
        "colab_type": "code",
        "outputId": "8b0a7eb8-75a7-4385-a280-64a91079f267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd pyGAT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pyGAT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLD8FQjbQVGo",
        "colab_type": "code",
        "outputId": "a50fde2c-d45b-4e0e-84e5-17d764c26f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqGMEdWOZ8GN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "path=\"./data/cora/\" \n",
        "dataset=\"cora\"\n",
        "idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "features = idx_features_labels[:, 1:-1].astype(bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZTJbOJX9njF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "    \n",
        "labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "\n",
        "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "idx_map = {j: i for i, j in enumerate(idx)}\n",
        "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ePBYJyoazq",
        "colab_type": "code",
        "outputId": "65196509-ae6f-4197-a66b-290346458b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx_test = range(500, 1500)\n",
        "features[idx_test].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1433)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Tny4uPOgP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_uniform(r, min_val, max_val):\n",
        "    point = 0\n",
        "    if (r <= 0):\n",
        "        point = -1.0\n",
        "    else:\n",
        "        log_min_val = np.log10(min_val)\n",
        "        log_max_val = np.log10(max_val)\n",
        "        point = 10.0 ** (log_min_val + r * (log_max_val - log_min_val))\n",
        "    return point"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUaWhZVkYEAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from utils import load_data, accuracy\n",
        "from models import GAT, SpGAT\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=True, help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
        "parser.add_argument('--sparse', action='store_true', default=True, help='GAT with sparse version or not.')\n",
        "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
        "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
        "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
        "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, help='noise level to add to test set')\n",
        "\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(noise_level_test = args.add_noise)\n",
        "\n",
        "# Model and optimizer\n",
        "if args.sparse:\n",
        "    model = SpGAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "else:\n",
        "    model = GAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=args.lr, \n",
        "                       weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    print('now adding noise to test data')\n",
        "    #output = add_noise(output, eps=0.1)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data[0]),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data[0]))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = args.epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(args.epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == args.patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model\n",
        "print('Loading {}th epoch'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO4n3g5WQvhT",
        "colab_type": "code",
        "outputId": "47e8a077-9fa3-4f99-ad8c-410661fbc510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from utils import load_data, accuracy\n",
        "from models import GAT, SpGAT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_test(noise_level):\n",
        "\n",
        "    acc = []\n",
        "    loss = []\n",
        "\n",
        "    for n in noise_level:\n",
        "        print(n)\n",
        "        # Load data with noise\n",
        "        print('now adding eps = ', n)\n",
        "        adj, features, labels, idx_train, idx_val, idx_test = load_data(noise_level_test = n)\n",
        "        \n",
        "        model.cuda()\n",
        "        features = features.cuda()\n",
        "        adj = adj.cuda()\n",
        "        labels = labels.cuda()\n",
        "        idx_train = idx_train.cuda()\n",
        "        idx_val = idx_val.cuda()\n",
        "        idx_test = idx_test.cuda()\n",
        "            \n",
        "        features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
        "\n",
        "        # model = SpGAT(nfeat=features.shape[1], \n",
        "        #               nhid=8, \n",
        "        #               nclass=int(labels.max()) + 1, \n",
        "        #               dropout=0.6, \n",
        "        #               nheads=8, \n",
        "        #               alpha=0.2)\n",
        "\n",
        "\n",
        "\n",
        "        # Restore best model\n",
        "        best_epoch = 99\n",
        "        print('Loading {}th epoch'.format(best_epoch))\n",
        "        model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        #output = add_noise(output, eps=0.1)\n",
        "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "        print(\"Test set results for eps = \", n, ' : ',\n",
        "              \"loss= {:.4f}\".format(loss_test.data[0]),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.data[0]))\n",
        "        \n",
        "        loss.append(loss_test.data[0])\n",
        "        acc.append(acc_test.data[0])\n",
        "\n",
        "    return loss,acc\n",
        "    \n",
        "\n",
        "# Testing\n",
        "\n",
        "noise_level = np.linspace(0, 0.9, num=20)\n",
        "\n",
        "loss,acc = compute_test(noise_level)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "now adding eps =  0.0\n",
            "Loading cora datasetss...\n",
            "Loading 99th epoch\n",
            "Test set results for eps =  0.0  :  loss= 1.0747 accuracy= 0.8280\n",
            "0.04736842105263158\n",
            "now adding eps =  0.04736842105263158\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.04736842105263158  :  loss= 1.4442 accuracy= 0.7670\n",
            "0.09473684210526316\n",
            "now adding eps =  0.09473684210526316\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.09473684210526316  :  loss= 1.4930 accuracy= 0.7510\n",
            "0.14210526315789473\n",
            "now adding eps =  0.14210526315789473\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.14210526315789473  :  loss= 1.5142 accuracy= 0.7320\n",
            "0.18947368421052632\n",
            "now adding eps =  0.18947368421052632\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.18947368421052632  :  loss= 1.5272 accuracy= 0.7120\n",
            "0.2368421052631579\n",
            "now adding eps =  0.2368421052631579\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.2368421052631579  :  loss= 1.5338 accuracy= 0.7110\n",
            "0.28421052631578947\n",
            "now adding eps =  0.28421052631578947\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.28421052631578947  :  loss= 1.5360 accuracy= 0.7070\n",
            "0.3315789473684211\n",
            "now adding eps =  0.3315789473684211\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.3315789473684211  :  loss= 1.5400 accuracy= 0.7060\n",
            "0.37894736842105264\n",
            "now adding eps =  0.37894736842105264\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.37894736842105264  :  loss= 1.5429 accuracy= 0.6970\n",
            "0.4263157894736842\n",
            "now adding eps =  0.4263157894736842\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.4263157894736842  :  loss= 1.5450 accuracy= 0.6980\n",
            "0.4736842105263158\n",
            "now adding eps =  0.4736842105263158\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.4736842105263158  :  loss= 1.5446 accuracy= 0.6970\n",
            "0.5210526315789474\n",
            "now adding eps =  0.5210526315789474\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.5210526315789474  :  loss= 1.5477 accuracy= 0.6940\n",
            "0.5684210526315789\n",
            "now adding eps =  0.5684210526315789\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.5684210526315789  :  loss= 1.5489 accuracy= 0.6950\n",
            "0.6157894736842106\n",
            "now adding eps =  0.6157894736842106\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.6157894736842106  :  loss= 1.5497 accuracy= 0.6900\n",
            "0.6631578947368422\n",
            "now adding eps =  0.6631578947368422\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.6631578947368422  :  loss= 1.5508 accuracy= 0.6920\n",
            "0.7105263157894737\n",
            "now adding eps =  0.7105263157894737\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.7105263157894737  :  loss= 1.5516 accuracy= 0.6920\n",
            "0.7578947368421053\n",
            "now adding eps =  0.7578947368421053\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.7578947368421053  :  loss= 1.5516 accuracy= 0.6980\n",
            "0.8052631578947369\n",
            "now adding eps =  0.8052631578947369\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.8052631578947369  :  loss= 1.5525 accuracy= 0.6900\n",
            "0.8526315789473684\n",
            "now adding eps =  0.8526315789473684\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.8526315789473684  :  loss= 1.5530 accuracy= 0.6900\n",
            "0.9\n",
            "now adding eps =  0.9\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 99th epoch\n",
            "Test set results for eps =  0.9  :  loss= 1.5531 accuracy= 0.6910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAK_ZGD5X5W0",
        "colab_type": "code",
        "outputId": "5b052730-ef32-4567-a17c-e3ca66f2e897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.plot(noise_level, acc, 'g')\n",
        "#plt.xscale('log')\n",
        "format_plot(x=r'$\\epsilon$', y='accuracy')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(noise_level, loss, 'r')\n",
        "\n",
        "format_plot(x=r'$\\epsilon$', y='loss')\n",
        "finalize_plot(shape=(1.2, 0.6))\n",
        "plt.savefig('acc-loss.pdf')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/pdf": "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\n",
            "text/plain": [
              "<Figure size 518.4x259.2 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"246.939687pt\" version=\"1.1\" viewBox=\"0 0 505.610937 246.939687\" width=\"505.610937pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 246.939687 \nL 505.610937 246.939687 \nL 505.610937 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.805938 203.98 \nL 249.065937 203.98 \nL 249.065937 7.2 \nL 56.805938 7.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 65.545028 203.98 \nL 65.545028 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 3.421875 \nQ 39.265625 3.421875 42.96875 11.625 \nQ 46.6875 19.828125 46.6875 36.375 \nQ 46.6875 52.984375 42.96875 61.1875 \nQ 39.265625 69.390625 31.78125 69.390625 \nQ 24.3125 69.390625 20.59375 61.1875 \nQ 16.890625 52.984375 16.890625 36.375 \nQ 16.890625 19.828125 20.59375 11.625 \nQ 24.3125 3.421875 31.78125 3.421875 \nz\nM 31.78125 -1.421875 \nQ 19.921875 -1.421875 13.25 8.53125 \nQ 6.59375 18.5 6.59375 36.375 \nQ 6.59375 54.296875 13.25 64.25 \nQ 19.921875 74.21875 31.78125 74.21875 \nQ 43.703125 74.21875 50.34375 64.25 \nQ 56.984375 54.296875 56.984375 36.375 \nQ 56.984375 18.5 50.34375 8.53125 \nQ 43.703125 -1.421875 31.78125 -1.421875 \nz\n\" id=\"DejaVuSerif-48\"/>\n       <path d=\"M 9.421875 5.078125 \nQ 9.421875 7.8125 11.28125 9.71875 \nQ 13.140625 11.625 15.921875 11.625 \nQ 18.609375 11.625 20.5 9.71875 \nQ 22.40625 7.8125 22.40625 5.078125 \nQ 22.40625 2.390625 20.5 0.484375 \nQ 18.609375 -1.421875 15.921875 -1.421875 \nQ 13.140625 -1.421875 11.28125 0.453125 \nQ 9.421875 2.34375 9.421875 5.078125 \nz\n\" id=\"DejaVuSerif-46\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(56.79831 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 104.385432 203.98 \nL 104.385432 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 12.796875 55.515625 \nL 7.328125 55.515625 \nL 7.328125 68.5 \nQ 12.546875 71.296875 17.84375 72.75 \nQ 23.140625 74.21875 28.21875 74.21875 \nQ 39.59375 74.21875 46.1875 68.703125 \nQ 52.78125 63.1875 52.78125 53.71875 \nQ 52.78125 43.015625 37.84375 28.125 \nQ 36.671875 27 36.078125 26.421875 \nL 17.671875 8.015625 \nL 48.09375 8.015625 \nL 48.09375 17 \nL 53.8125 17 \nL 53.8125 0 \nL 6.78125 0 \nL 6.78125 5.328125 \nL 28.90625 27.390625 \nQ 36.234375 34.71875 39.359375 40.84375 \nQ 42.484375 46.96875 42.484375 53.71875 \nQ 42.484375 61.078125 38.640625 65.234375 \nQ 34.8125 69.390625 28.078125 69.390625 \nQ 21.09375 69.390625 17.28125 65.921875 \nQ 13.484375 62.453125 12.796875 55.515625 \nz\n\" id=\"DejaVuSerif-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(95.638714 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 143.225836 203.98 \nL 143.225836 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 34.90625 24.703125 \nL 34.90625 63.484375 \nL 10.015625 24.703125 \nz\nM 56.390625 0 \nL 23.1875 0 \nL 23.1875 5.171875 \nL 34.90625 5.171875 \nL 34.90625 19.484375 \nL 3.078125 19.484375 \nL 3.078125 24.8125 \nL 35.015625 74.21875 \nL 44.671875 74.21875 \nL 44.671875 24.703125 \nL 58.59375 24.703125 \nL 58.59375 19.484375 \nL 44.671875 19.484375 \nL 44.671875 5.171875 \nL 56.390625 5.171875 \nz\n\" id=\"DejaVuSerif-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(134.479118 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 182.066241 203.98 \nL 182.066241 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 32.71875 3.421875 \nQ 39.59375 3.421875 43.296875 8.46875 \nQ 47.015625 13.53125 47.015625 23 \nQ 47.015625 32.46875 43.296875 37.515625 \nQ 39.59375 42.578125 32.71875 42.578125 \nQ 25.734375 42.578125 22.0625 37.6875 \nQ 18.40625 32.8125 18.40625 23.578125 \nQ 18.40625 13.875 22.109375 8.640625 \nQ 25.828125 3.421875 32.71875 3.421875 \nz\nM 16.796875 40.140625 \nQ 20.125 43.796875 24.3125 45.59375 \nQ 28.515625 47.40625 33.796875 47.40625 \nQ 44.671875 47.40625 51 40.859375 \nQ 57.328125 34.328125 57.328125 23 \nQ 57.328125 11.921875 50.515625 5.25 \nQ 43.703125 -1.421875 32.328125 -1.421875 \nQ 19.96875 -1.421875 13.328125 7.78125 \nQ 6.6875 17 6.6875 34.078125 \nQ 6.6875 53.21875 14.546875 63.71875 \nQ 22.40625 74.21875 36.71875 74.21875 \nQ 40.578125 74.21875 44.828125 73.484375 \nQ 49.078125 72.75 53.515625 71.296875 \nL 53.515625 59.28125 \nL 48 59.28125 \nQ 47.40625 64.203125 44.234375 66.796875 \nQ 41.0625 69.390625 35.6875 69.390625 \nQ 26.21875 69.390625 21.578125 62.203125 \nQ 16.9375 55.03125 16.796875 40.140625 \nz\n\" id=\"DejaVuSerif-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(173.319522 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 220.906645 203.98 \nL 220.906645 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 46.578125 19.921875 \nQ 46.578125 27.734375 42.6875 32.046875 \nQ 38.8125 36.375 31.78125 36.375 \nQ 24.75 36.375 20.875 32.046875 \nQ 17 27.734375 17 19.921875 \nQ 17 12.0625 20.875 7.734375 \nQ 24.75 3.421875 31.78125 3.421875 \nQ 38.8125 3.421875 42.6875 7.734375 \nQ 46.578125 12.0625 46.578125 19.921875 \nz\nM 44.578125 55.328125 \nQ 44.578125 61.96875 41.203125 65.671875 \nQ 37.84375 69.390625 31.78125 69.390625 \nQ 25.78125 69.390625 22.390625 65.671875 \nQ 19 61.96875 19 55.328125 \nQ 19 48.640625 22.390625 44.921875 \nQ 25.78125 41.21875 31.78125 41.21875 \nQ 37.84375 41.21875 41.203125 44.921875 \nQ 44.578125 48.640625 44.578125 55.328125 \nz\nM 39.3125 38.8125 \nQ 47.609375 37.703125 52.25 32.6875 \nQ 56.890625 27.6875 56.890625 19.921875 \nQ 56.890625 9.671875 50.390625 4.125 \nQ 43.890625 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.203125 4.125 \nQ 6.6875 9.671875 6.6875 19.921875 \nQ 6.6875 27.6875 11.328125 32.6875 \nQ 15.96875 37.703125 24.3125 38.8125 \nQ 16.9375 40.140625 13 44.40625 \nQ 9.078125 48.6875 9.078125 55.328125 \nQ 9.078125 64.109375 15.125 69.15625 \nQ 21.1875 74.21875 31.78125 74.21875 \nQ 42.390625 74.21875 48.4375 69.15625 \nQ 54.5 64.109375 54.5 55.328125 \nQ 54.5 48.6875 50.5625 44.40625 \nQ 46.625 40.140625 39.3125 38.8125 \nz\n\" id=\"DejaVuSerif-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(212.159926 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $\\epsilon$ -->\n     <defs>\n      <path d=\"M 19.734375 29.5 \nQ 14.453125 30.671875 12.15625 33.84375 \nQ 10.453125 36.078125 10.453125 39.109375 \nQ 10.453125 47.40625 18.359375 52.25 \nQ 24.609375 56.0625 34.1875 56.0625 \nQ 37.890625 56.0625 41.9375 55.46875 \nQ 46 54.890625 50.53125 53.71875 \nL 48.96875 45.5625 \nQ 44.484375 46.96875 40.71875 47.609375 \nQ 36.859375 48.25 33.40625 48.25 \nQ 27.59375 48.25 23.78125 46 \nQ 19.1875 43.3125 19.1875 39.40625 \nQ 19.1875 36.8125 21.578125 35.015625 \nQ 24.421875 32.859375 30.078125 32.859375 \nL 37.640625 32.859375 \nL 36.234375 25.4375 \nL 29 25.4375 \nQ 22.265625 25.4375 18.3125 22.953125 \nQ 12.9375 19.578125 12.9375 14.3125 \nQ 12.9375 10.984375 15.828125 8.796875 \nQ 19.4375 6.0625 26.8125 6.0625 \nQ 31.34375 6.0625 35.6875 6.9375 \nQ 40.046875 7.859375 43.84375 9.671875 \nL 42.1875 1.3125 \nQ 37.546875 -0.046875 33.296875 -0.734375 \nQ 29.046875 -1.421875 25.140625 -1.421875 \nQ 13.53125 -1.421875 8.0625 3.03125 \nQ 3.90625 6.453125 3.90625 12.203125 \nQ 3.90625 19.96875 9.375 24.75 \nQ 13.421875 28.328125 19.734375 29.5 \nz\n\" id=\"DejaVuSans-Oblique-949\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(149.635937 237.244062)scale(0.12 -0.12)\">\n      <use transform=\"translate(0 0.9375)\" xlink:href=\"#DejaVuSans-Oblique-949\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 56.805938 182.072345 \nL 249.065937 182.072345 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.70 -->\n      <defs>\n       <path d=\"M 56.390625 67.921875 \nL 27.875 0 \nL 20.609375 0 \nL 47.796875 64.890625 \nL 14.109375 64.890625 \nL 14.109375 55.90625 \nL 8.40625 55.90625 \nL 8.40625 72.90625 \nL 56.390625 72.90625 \nz\n\" id=\"DejaVuSerif-55\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 186.251486)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 56.805938 156.146126 \nL 249.065937 156.146126 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.72 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 160.325267)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 56.805938 130.219908 \nL 249.065937 130.219908 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.74 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 134.399048)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 56.805938 104.293689 \nL 249.065937 104.293689 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.76 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 108.47283)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 56.805938 78.36747 \nL 249.065937 78.36747 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.78 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 82.546611)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 56.805938 52.441252 \nL 249.065937 52.441252 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.80 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 56.620392)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 56.805938 26.515033 \nL 249.065937 26.515033 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.82 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 30.694174)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- accuracy -->\n     <defs>\n      <path d=\"M 39.796875 16.3125 \nL 39.796875 27.296875 \nL 28.21875 27.296875 \nQ 21.53125 27.296875 18.25 24.40625 \nQ 14.984375 21.53125 14.984375 15.578125 \nQ 14.984375 10.15625 18.296875 6.984375 \nQ 21.625 3.8125 27.296875 3.8125 \nQ 32.90625 3.8125 36.34375 7.28125 \nQ 39.796875 10.75 39.796875 16.3125 \nz\nM 48.78125 32.421875 \nL 48.78125 5.171875 \nL 56.78125 5.171875 \nL 56.78125 0 \nL 39.796875 0 \nL 39.796875 5.609375 \nQ 36.8125 2 32.90625 0.28125 \nQ 29 -1.421875 23.78125 -1.421875 \nQ 15.140625 -1.421875 10.0625 3.171875 \nQ 4.984375 7.765625 4.984375 15.578125 \nQ 4.984375 23.640625 10.796875 28.078125 \nQ 16.609375 32.515625 27.203125 32.515625 \nL 39.796875 32.515625 \nL 39.796875 36.078125 \nQ 39.796875 42 36.203125 45.234375 \nQ 32.625 48.484375 26.125 48.484375 \nQ 20.75 48.484375 17.578125 46.046875 \nQ 14.40625 43.609375 13.625 38.8125 \nL 8.984375 38.8125 \nL 8.984375 49.3125 \nQ 13.671875 51.3125 18.09375 52.3125 \nQ 22.515625 53.328125 26.703125 53.328125 \nQ 37.5 53.328125 43.140625 47.96875 \nQ 48.78125 42.625 48.78125 32.421875 \nz\n\" id=\"DejaVuSerif-97\"/>\n      <path d=\"M 51.421875 15.578125 \nQ 49.515625 7.28125 44.09375 2.921875 \nQ 38.671875 -1.421875 30.078125 -1.421875 \nQ 18.75 -1.421875 11.859375 6.078125 \nQ 4.984375 13.578125 4.984375 25.984375 \nQ 4.984375 38.421875 11.859375 45.875 \nQ 18.75 53.328125 30.078125 53.328125 \nQ 35.015625 53.328125 39.890625 52.171875 \nQ 44.78125 51.03125 49.703125 48.6875 \nL 49.703125 35.40625 \nL 44.484375 35.40625 \nQ 43.453125 42.234375 40.015625 45.359375 \nQ 36.578125 48.484375 30.171875 48.484375 \nQ 22.90625 48.484375 19.1875 42.84375 \nQ 15.484375 37.203125 15.484375 25.984375 \nQ 15.484375 14.75 19.171875 9.078125 \nQ 22.859375 3.421875 30.171875 3.421875 \nQ 35.984375 3.421875 39.453125 6.4375 \nQ 42.921875 9.46875 44.1875 15.578125 \nz\n\" id=\"DejaVuSerif-99\"/>\n      <path d=\"M 35.40625 51.90625 \nL 52.203125 51.90625 \nL 52.203125 5.171875 \nL 60.6875 5.171875 \nL 60.6875 0 \nL 43.21875 0 \nL 43.21875 9.1875 \nQ 40.71875 4 36.765625 1.28125 \nQ 32.8125 -1.421875 27.59375 -1.421875 \nQ 18.953125 -1.421875 14.875 3.484375 \nQ 10.796875 8.40625 10.796875 18.890625 \nL 10.796875 46.6875 \nL 2.6875 46.6875 \nL 2.6875 51.90625 \nL 19.828125 51.90625 \nL 19.828125 21.6875 \nQ 19.828125 12.203125 22.140625 8.6875 \nQ 24.46875 5.171875 30.421875 5.171875 \nQ 36.671875 5.171875 39.9375 9.765625 \nQ 43.21875 14.359375 43.21875 23.09375 \nL 43.21875 46.6875 \nL 35.40625 46.6875 \nz\n\" id=\"DejaVuSerif-117\"/>\n      <path d=\"M 47.796875 52 \nL 47.796875 39.015625 \nL 42.625 39.015625 \nQ 42.390625 42.875 40.484375 44.78125 \nQ 38.578125 46.6875 34.90625 46.6875 \nQ 28.265625 46.6875 24.71875 42.09375 \nQ 21.1875 37.5 21.1875 28.90625 \nL 21.1875 5.171875 \nL 31.59375 5.171875 \nL 31.59375 0 \nL 4.109375 0 \nL 4.109375 5.171875 \nL 12.203125 5.171875 \nL 12.203125 46.78125 \nL 3.609375 46.78125 \nL 3.609375 51.90625 \nL 21.1875 51.90625 \nL 21.1875 42.671875 \nQ 23.828125 48.09375 27.96875 50.703125 \nQ 32.125 53.328125 38.09375 53.328125 \nQ 40.28125 53.328125 42.703125 52.984375 \nQ 45.125 52.640625 47.796875 52 \nz\n\" id=\"DejaVuSerif-114\"/>\n      <path d=\"M 21.578125 -9.515625 \nL 25 -0.875 \nL 5.609375 46.6875 \nL -0.296875 46.6875 \nL -0.296875 51.90625 \nL 23.578125 51.90625 \nL 23.578125 46.6875 \nL 15.28125 46.6875 \nL 29.890625 10.984375 \nL 44.484375 46.6875 \nL 36.71875 46.6875 \nL 36.71875 51.90625 \nL 56.203125 51.90625 \nL 56.203125 46.6875 \nL 50.390625 46.6875 \nL 26.609375 -11.71875 \nQ 24.171875 -17.78125 21.1875 -20 \nQ 18.21875 -22.21875 12.796875 -22.21875 \nQ 10.5 -22.21875 8.078125 -21.828125 \nQ 5.671875 -21.4375 3.21875 -20.703125 \nL 3.21875 -10.796875 \nL 7.8125 -10.796875 \nQ 8.109375 -14.109375 9.5 -15.546875 \nQ 10.890625 -17 13.8125 -17 \nQ 16.5 -17 18.140625 -15.5 \nQ 19.78125 -14.015625 21.578125 -9.515625 \nz\n\" id=\"DejaVuSerif-121\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.1475 132.947188)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSerif-97\"/>\n      <use x=\"59.619141\" xlink:href=\"#DejaVuSerif-99\"/>\n      <use x=\"115.625\" xlink:href=\"#DejaVuSerif-99\"/>\n      <use x=\"171.630859\" xlink:href=\"#DejaVuSerif-117\"/>\n      <use x=\"236.035156\" xlink:href=\"#DejaVuSerif-114\"/>\n      <use x=\"283.837891\" xlink:href=\"#DejaVuSerif-97\"/>\n      <use x=\"343.457031\" xlink:href=\"#DejaVuSerif-99\"/>\n      <use x=\"399.462891\" xlink:href=\"#DejaVuSerif-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p8dd1f865fb)\" d=\"M 65.545028 16.144545 \nL 74.744071 95.219513 \nL 83.943115 115.960487 \nL 93.142158 140.590395 \nL 102.341201 166.516614 \nL 111.540244 167.812925 \nL 120.739287 172.998169 \nL 129.93833 174.29448 \nL 139.137373 185.961278 \nL 148.336416 184.664967 \nL 157.535459 185.961278 \nL 166.734502 189.850211 \nL 175.933545 188.5539 \nL 185.132588 195.035455 \nL 194.331631 192.442833 \nL 203.530674 192.442833 \nL 212.729717 184.664967 \nL 221.92876 195.035455 \nL 231.127804 195.035455 \nL 240.326847 193.739144 \n\" style=\"fill:none;stroke:#55a868;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.805938 203.98 \nL 56.805938 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 249.065937 203.98 \nL 249.065937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.805938 203.98 \nL 249.065937 203.98 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.805938 7.2 \nL 249.065937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 306.150937 203.98 \nL 498.410937 203.98 \nL 498.410937 7.2 \nL 306.150937 7.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_6\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 314.890028 203.98 \nL 314.890028 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(306.14331 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 353.730432 203.98 \nL 353.730432 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(344.983714 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_16\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 392.570836 203.98 \nL 392.570836 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(383.824118 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 431.411241 203.98 \nL 431.411241 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0.6 -->\n      <g style=\"fill:#262626;\" transform=\"translate(422.664522 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_18\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 470.251645 203.98 \nL 470.251645 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0.8 -->\n      <g style=\"fill:#262626;\" transform=\"translate(461.504926 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- $\\epsilon$ -->\n     <g style=\"fill:#262626;\" transform=\"translate(398.980937 237.244062)scale(0.12 -0.12)\">\n      <use transform=\"translate(0 0.9375)\" xlink:href=\"#DejaVuSans-Oblique-949\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_8\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 306.150937 185.586163 \nL 498.410937 185.586163 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_21\">\n      <!-- 1.1 -->\n      <defs>\n       <path d=\"M 14.203125 0 \nL 14.203125 5.171875 \nL 26.90625 5.171875 \nL 26.90625 65.828125 \nL 12.203125 56.296875 \nL 12.203125 62.703125 \nL 29.984375 74.21875 \nL 36.71875 74.21875 \nL 36.71875 5.171875 \nL 49.421875 5.171875 \nL 49.421875 0 \nz\n\" id=\"DejaVuSerif-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(279.1575 189.765304)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_20\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 306.150937 148.186614 \nL 498.410937 148.186614 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_22\">\n      <!-- 1.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(279.1575 152.365754)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 306.150937 110.787064 \nL 498.410937 110.787064 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_23\">\n      <!-- 1.3 -->\n      <defs>\n       <path d=\"M 9.71875 69.828125 \nQ 15.4375 71.96875 20.671875 73.09375 \nQ 25.921875 74.21875 30.515625 74.21875 \nQ 41.21875 74.21875 47.21875 69.59375 \nQ 53.21875 64.984375 53.21875 56.78125 \nQ 53.21875 50.203125 49.0625 45.78125 \nQ 44.921875 41.359375 37.3125 39.796875 \nQ 46.296875 38.53125 51.25 33.28125 \nQ 56.203125 28.03125 56.203125 19.671875 \nQ 56.203125 9.46875 49.34375 4.015625 \nQ 42.484375 -1.421875 29.59375 -1.421875 \nQ 23.875 -1.421875 18.421875 -0.1875 \nQ 12.984375 1.03125 7.625 3.515625 \nL 7.625 17.671875 \nL 13.09375 17.671875 \nQ 13.578125 10.640625 17.828125 7.03125 \nQ 22.078125 3.421875 29.78125 3.421875 \nQ 37.25 3.421875 41.578125 7.734375 \nQ 45.90625 12.0625 45.90625 19.578125 \nQ 45.90625 28.171875 41.453125 32.59375 \nQ 37.015625 37.015625 28.421875 37.015625 \nL 23.78125 37.015625 \nL 23.78125 42 \nL 26.21875 42 \nQ 34.765625 42 39.03125 45.53125 \nQ 43.3125 49.078125 43.3125 56.203125 \nQ 43.3125 62.59375 39.796875 65.984375 \nQ 36.28125 69.390625 29.6875 69.390625 \nQ 23.09375 69.390625 19.453125 66.265625 \nQ 15.828125 63.140625 15.1875 56.984375 \nL 9.71875 56.984375 \nz\n\" id=\"DejaVuSerif-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(279.1575 114.966204)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_22\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 306.150937 73.387514 \nL 498.410937 73.387514 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_24\">\n      <!-- 1.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(279.1575 77.566655)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 306.150937 35.987964 \nL 498.410937 35.987964 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_25\">\n      <!-- 1.5 -->\n      <defs>\n       <path d=\"M 50.296875 72.90625 \nL 50.296875 64.890625 \nL 16.890625 64.890625 \nL 16.890625 44 \nQ 19.4375 45.75 22.828125 46.625 \nQ 26.21875 47.515625 30.421875 47.515625 \nQ 42.234375 47.515625 49.0625 40.96875 \nQ 55.90625 34.421875 55.90625 23.09375 \nQ 55.90625 11.53125 49 5.046875 \nQ 42.09375 -1.421875 29.59375 -1.421875 \nQ 24.5625 -1.421875 19.28125 -0.1875 \nQ 14.015625 1.03125 8.5 3.515625 \nL 8.5 17.671875 \nL 14.015625 17.671875 \nQ 14.453125 10.75 18.421875 7.078125 \nQ 22.40625 3.421875 29.59375 3.421875 \nQ 37.3125 3.421875 41.453125 8.5 \nQ 45.609375 13.578125 45.609375 23.09375 \nQ 45.609375 32.5625 41.484375 37.609375 \nQ 37.359375 42.671875 29.59375 42.671875 \nQ 25.203125 42.671875 21.84375 41.109375 \nQ 18.5 39.546875 15.921875 36.28125 \nL 11.71875 36.28125 \nL 11.71875 72.90625 \nz\n\" id=\"DejaVuSerif-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(279.1575 40.167105)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_26\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 20.515625 5.171875 \nL 29 5.171875 \nL 29 0 \nL 2.875 0 \nL 2.875 5.171875 \nL 11.53125 5.171875 \nL 11.53125 70.796875 \nL 2.875 70.796875 \nL 2.875 75.984375 \nL 20.515625 75.984375 \nz\n\" id=\"DejaVuSerif-108\"/>\n      <path d=\"M 30.078125 3.421875 \nQ 37.3125 3.421875 40.984375 9.125 \nQ 44.671875 14.84375 44.671875 25.984375 \nQ 44.671875 37.109375 40.984375 42.796875 \nQ 37.3125 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 19.171875 42.796875 \nQ 15.484375 37.109375 15.484375 25.984375 \nQ 15.484375 14.84375 19.1875 9.125 \nQ 22.90625 3.421875 30.078125 3.421875 \nz\nM 30.078125 -1.421875 \nQ 18.75 -1.421875 11.859375 6.078125 \nQ 4.984375 13.578125 4.984375 25.984375 \nQ 4.984375 38.375 11.84375 45.84375 \nQ 18.703125 53.328125 30.078125 53.328125 \nQ 41.453125 53.328125 48.3125 45.84375 \nQ 55.171875 38.375 55.171875 25.984375 \nQ 55.171875 13.578125 48.3125 6.078125 \nQ 41.453125 -1.421875 30.078125 -1.421875 \nz\n\" id=\"DejaVuSerif-111\"/>\n      <path d=\"M 5.609375 2.875 \nL 5.609375 14.984375 \nL 10.796875 14.984375 \nQ 10.984375 9.1875 14.421875 6.296875 \nQ 17.875 3.421875 24.609375 3.421875 \nQ 30.671875 3.421875 33.84375 5.6875 \nQ 37.015625 7.953125 37.015625 12.3125 \nQ 37.015625 15.71875 34.6875 17.8125 \nQ 32.375 19.921875 24.90625 22.3125 \nL 18.40625 24.515625 \nQ 11.71875 26.65625 8.71875 29.875 \nQ 5.71875 33.109375 5.71875 38.09375 \nQ 5.71875 45.21875 10.9375 49.265625 \nQ 16.15625 53.328125 25.390625 53.328125 \nQ 29.5 53.328125 34.03125 52.25 \nQ 38.578125 51.171875 43.40625 49.125 \nL 43.40625 37.796875 \nL 38.234375 37.796875 \nQ 38.03125 42.828125 34.703125 45.65625 \nQ 31.390625 48.484375 25.6875 48.484375 \nQ 20.015625 48.484375 17.109375 46.484375 \nQ 14.203125 44.484375 14.203125 40.484375 \nQ 14.203125 37.203125 16.40625 35.21875 \nQ 18.609375 33.25 25.203125 31.203125 \nL 32.328125 29 \nQ 39.703125 26.703125 42.9375 23.265625 \nQ 46.1875 19.828125 46.1875 14.40625 \nQ 46.1875 7.03125 40.546875 2.796875 \nQ 34.90625 -1.421875 25 -1.421875 \nQ 19.96875 -1.421875 15.1875 -0.34375 \nQ 10.40625 0.734375 5.609375 2.875 \nz\n\" id=\"DejaVuSerif-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(272.661875 117.27875)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSerif-108\"/>\n      <use x=\"31.982422\" xlink:href=\"#DejaVuSerif-111\"/>\n      <use x=\"92.1875\" xlink:href=\"#DejaVuSerif-115\"/>\n      <use x=\"143.505859\" xlink:href=\"#DejaVuSerif-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_24\">\n    <path clip-path=\"url(#p15cdb0e33e)\" d=\"M 314.890028 195.035455 \nL 324.089071 56.875356 \nL 333.288115 38.590317 \nL 342.487158 30.688073 \nL 351.686201 25.813911 \nL 360.885244 23.343615 \nL 370.084287 22.538255 \nL 379.28333 21.027044 \nL 388.482373 19.954493 \nL 397.681416 19.15426 \nL 406.880459 19.323723 \nL 416.079502 18.15875 \nL 425.278545 17.699091 \nL 434.477588 17.391508 \nL 443.676631 16.978529 \nL 452.875674 16.674646 \nL 462.074717 16.703135 \nL 471.27376 16.362114 \nL 480.472804 16.151768 \nL 489.671847 16.144545 \n\" style=\"fill:none;stroke:#c44e52;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 306.150937 203.98 \nL 306.150937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 498.410937 203.98 \nL 498.410937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 306.150937 203.98 \nL 498.410937 203.98 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 306.150937 7.2 \nL 498.410937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8dd1f865fb\">\n   <rect height=\"196.78\" width=\"192.26\" x=\"56.805938\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p15cdb0e33e\">\n   <rect height=\"196.78\" width=\"192.26\" x=\"306.150937\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxIHmktJU8B5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ab4236d-44af-47e6-dbc7-e11f3943a981"
      },
      "source": [
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('pdf', 'svg')\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "\n",
        "import functools\n",
        "\n",
        "  \n",
        "sns.set(font_scale=1.3)\n",
        "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".95\"})\n",
        "sns.set(font='serif')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def format_plot(x=None, y=None): \n",
        "  # plt.grid(False)\n",
        "  ax = plt.gca()\n",
        "  if x is not None:\n",
        "    plt.xlabel(x, fontsize=12)\n",
        "  if y is not None:\n",
        "    plt.ylabel(y, fontsize=12)\n",
        "  \n",
        "def finalize_plot(shape=(1, 1)):\n",
        "  plt.gcf().set_size_inches(\n",
        "    shape[0] * 1.5 * plt.gcf().get_size_inches()[1], \n",
        "    shape[1] * 1.5 * plt.gcf().get_size_inches()[1])\n",
        "  plt.tight_layout()\n",
        "\n",
        "legend = functools.partial(plt.legend, fontsize=10)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwVOYGO1R_hI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03cf0872-9cf0-41d9-e1ce-69dcef1e9fe4"
      },
      "source": [
        "# Now train with noise\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "from utils import load_data, accuracy\n",
        "from models import GAT, SpGAT\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=True, help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
        "parser.add_argument('--sparse', action='store_true', default=True, help='GAT with sparse version or not.')\n",
        "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=250, help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
        "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
        "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
        "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, help='noise level to add to test set')\n",
        "\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model and optimizer\n",
        "if args.sparse:\n",
        "    model = SpGAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "else:\n",
        "    model = GAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=args.lr, \n",
        "                       weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data[0]),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data[0]))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = args.epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(args.epochs):\n",
        "\n",
        "    # re-generate noisy data\n",
        "    r = np.random.rand()\n",
        "    noise_level_train = log_uniform(r, 0.05, 0.6)\n",
        "\n",
        "\n",
        "    # Load data\n",
        "    adj, features, labels, idx_train, idx_val, idx_test = load_data(noise_level_test = args.add_noise)\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "    features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
        "\n",
        "\n",
        "    loss_values.append(train(epoch))\n",
        "\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == args.patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model\n",
        "print('Loading {}th epoch'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora datasetss...\n",
            "Epoch: 0001 loss_train: 1.9518 acc_train: 0.0714 loss_val: 1.9389 acc_val: 0.3000 time: 0.2606s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0002 loss_train: 1.9412 acc_train: 0.1857 loss_val: 1.9305 acc_val: 0.4500 time: 0.2619s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0003 loss_train: 1.9305 acc_train: 0.2500 loss_val: 1.9223 acc_val: 0.4933 time: 0.2452s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0004 loss_train: 1.9154 acc_train: 0.3857 loss_val: 1.9139 acc_val: 0.5167 time: 0.2296s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0005 loss_train: 1.9090 acc_train: 0.3857 loss_val: 1.9054 acc_val: 0.5267 time: 0.2181s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0006 loss_train: 1.8994 acc_train: 0.4500 loss_val: 1.8969 acc_val: 0.5300 time: 0.2119s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0007 loss_train: 1.8880 acc_train: 0.4929 loss_val: 1.8884 acc_val: 0.5300 time: 0.2076s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0008 loss_train: 1.8778 acc_train: 0.4714 loss_val: 1.8797 acc_val: 0.5400 time: 0.2065s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0009 loss_train: 1.8636 acc_train: 0.4786 loss_val: 1.8707 acc_val: 0.5400 time: 0.2076s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0010 loss_train: 1.8582 acc_train: 0.5429 loss_val: 1.8615 acc_val: 0.5467 time: 0.2083s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0011 loss_train: 1.8441 acc_train: 0.4714 loss_val: 1.8520 acc_val: 0.5567 time: 0.2538s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0012 loss_train: 1.8458 acc_train: 0.5071 loss_val: 1.8424 acc_val: 0.5567 time: 0.2307s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0013 loss_train: 1.8206 acc_train: 0.5214 loss_val: 1.8325 acc_val: 0.5567 time: 0.2190s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0014 loss_train: 1.7977 acc_train: 0.5714 loss_val: 1.8223 acc_val: 0.5467 time: 0.2103s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0015 loss_train: 1.7883 acc_train: 0.4929 loss_val: 1.8118 acc_val: 0.5433 time: 0.2089s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0016 loss_train: 1.7982 acc_train: 0.5000 loss_val: 1.8012 acc_val: 0.5400 time: 0.2069s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0017 loss_train: 1.7698 acc_train: 0.5286 loss_val: 1.7904 acc_val: 0.5433 time: 0.2059s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0018 loss_train: 1.7719 acc_train: 0.5143 loss_val: 1.7795 acc_val: 0.5400 time: 0.2237s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0019 loss_train: 1.7591 acc_train: 0.5357 loss_val: 1.7684 acc_val: 0.5433 time: 0.2342s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0020 loss_train: 1.7428 acc_train: 0.5214 loss_val: 1.7573 acc_val: 0.5467 time: 0.2620s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0021 loss_train: 1.7338 acc_train: 0.5214 loss_val: 1.7460 acc_val: 0.5433 time: 0.2557s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0022 loss_train: 1.7194 acc_train: 0.5286 loss_val: 1.7346 acc_val: 0.5433 time: 0.2342s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0023 loss_train: 1.6844 acc_train: 0.5643 loss_val: 1.7230 acc_val: 0.5400 time: 0.2269s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0024 loss_train: 1.6805 acc_train: 0.5500 loss_val: 1.7112 acc_val: 0.5433 time: 0.2201s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0025 loss_train: 1.6578 acc_train: 0.5500 loss_val: 1.6993 acc_val: 0.5467 time: 0.2139s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0026 loss_train: 1.6240 acc_train: 0.5000 loss_val: 1.6871 acc_val: 0.5567 time: 0.2154s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0027 loss_train: 1.6468 acc_train: 0.5429 loss_val: 1.6750 acc_val: 0.5633 time: 0.2130s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0028 loss_train: 1.6384 acc_train: 0.5500 loss_val: 1.6628 acc_val: 0.5800 time: 0.2535s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0029 loss_train: 1.5627 acc_train: 0.6143 loss_val: 1.6506 acc_val: 0.5833 time: 0.2508s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0030 loss_train: 1.6125 acc_train: 0.5929 loss_val: 1.6383 acc_val: 0.5867 time: 0.2343s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0031 loss_train: 1.5331 acc_train: 0.5429 loss_val: 1.6261 acc_val: 0.5867 time: 0.2327s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0032 loss_train: 1.5701 acc_train: 0.5857 loss_val: 1.6137 acc_val: 0.6033 time: 0.2292s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0033 loss_train: 1.5809 acc_train: 0.5214 loss_val: 1.6015 acc_val: 0.6033 time: 0.2208s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0034 loss_train: 1.5461 acc_train: 0.5500 loss_val: 1.5892 acc_val: 0.6067 time: 0.2196s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0035 loss_train: 1.5748 acc_train: 0.5214 loss_val: 1.5770 acc_val: 0.6100 time: 0.2087s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0036 loss_train: 1.5294 acc_train: 0.6357 loss_val: 1.5649 acc_val: 0.6200 time: 0.2101s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0037 loss_train: 1.5292 acc_train: 0.6071 loss_val: 1.5527 acc_val: 0.6267 time: 0.2574s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0038 loss_train: 1.5090 acc_train: 0.6286 loss_val: 1.5407 acc_val: 0.6367 time: 0.2438s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0039 loss_train: 1.5119 acc_train: 0.5786 loss_val: 1.5287 acc_val: 0.6500 time: 0.2316s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0040 loss_train: 1.4717 acc_train: 0.6214 loss_val: 1.5169 acc_val: 0.6600 time: 0.2301s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0041 loss_train: 1.4728 acc_train: 0.6000 loss_val: 1.5051 acc_val: 0.6700 time: 0.2121s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0042 loss_train: 1.4422 acc_train: 0.6429 loss_val: 1.4934 acc_val: 0.6700 time: 0.2090s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0043 loss_train: 1.4566 acc_train: 0.6286 loss_val: 1.4816 acc_val: 0.6733 time: 0.2170s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0044 loss_train: 1.4344 acc_train: 0.5929 loss_val: 1.4701 acc_val: 0.6767 time: 0.2138s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0045 loss_train: 1.4522 acc_train: 0.6214 loss_val: 1.4587 acc_val: 0.6833 time: 0.2092s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0046 loss_train: 1.3989 acc_train: 0.6714 loss_val: 1.4474 acc_val: 0.6833 time: 0.2652s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0047 loss_train: 1.3746 acc_train: 0.6429 loss_val: 1.4362 acc_val: 0.6900 time: 0.2496s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0048 loss_train: 1.3612 acc_train: 0.6929 loss_val: 1.4250 acc_val: 0.6933 time: 0.2340s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0049 loss_train: 1.4071 acc_train: 0.6071 loss_val: 1.4141 acc_val: 0.7067 time: 0.2224s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0050 loss_train: 1.3311 acc_train: 0.6429 loss_val: 1.4034 acc_val: 0.7133 time: 0.2133s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0051 loss_train: 1.3021 acc_train: 0.7214 loss_val: 1.3928 acc_val: 0.7133 time: 0.2070s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0052 loss_train: 1.2985 acc_train: 0.6714 loss_val: 1.3823 acc_val: 0.7200 time: 0.2093s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0053 loss_train: 1.3607 acc_train: 0.6786 loss_val: 1.3719 acc_val: 0.7267 time: 0.2068s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0054 loss_train: 1.2889 acc_train: 0.7286 loss_val: 1.3615 acc_val: 0.7300 time: 0.2127s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0055 loss_train: 1.2664 acc_train: 0.7143 loss_val: 1.3510 acc_val: 0.7367 time: 0.2559s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0056 loss_train: 1.3249 acc_train: 0.6714 loss_val: 1.3405 acc_val: 0.7467 time: 0.2395s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0057 loss_train: 1.2349 acc_train: 0.7714 loss_val: 1.3303 acc_val: 0.7500 time: 0.2223s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0058 loss_train: 1.2587 acc_train: 0.7071 loss_val: 1.3201 acc_val: 0.7533 time: 0.2143s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0059 loss_train: 1.2565 acc_train: 0.7214 loss_val: 1.3100 acc_val: 0.7567 time: 0.2125s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0060 loss_train: 1.2878 acc_train: 0.7000 loss_val: 1.3000 acc_val: 0.7767 time: 0.2119s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0061 loss_train: 1.2650 acc_train: 0.7000 loss_val: 1.2904 acc_val: 0.7900 time: 0.2076s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0062 loss_train: 1.1860 acc_train: 0.7714 loss_val: 1.2806 acc_val: 0.8000 time: 0.2101s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0063 loss_train: 1.2350 acc_train: 0.7071 loss_val: 1.2708 acc_val: 0.8033 time: 0.2068s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0064 loss_train: 1.2080 acc_train: 0.7286 loss_val: 1.2611 acc_val: 0.8100 time: 0.2602s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0065 loss_train: 1.2122 acc_train: 0.7000 loss_val: 1.2517 acc_val: 0.8067 time: 0.2422s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0066 loss_train: 1.2038 acc_train: 0.7143 loss_val: 1.2423 acc_val: 0.8067 time: 0.2281s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0067 loss_train: 1.1658 acc_train: 0.7143 loss_val: 1.2329 acc_val: 0.8067 time: 0.2381s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0068 loss_train: 1.1574 acc_train: 0.7357 loss_val: 1.2239 acc_val: 0.8067 time: 0.2192s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0069 loss_train: 1.2415 acc_train: 0.7071 loss_val: 1.2149 acc_val: 0.8100 time: 0.2065s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0070 loss_train: 1.1932 acc_train: 0.7357 loss_val: 1.2060 acc_val: 0.8100 time: 0.2127s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0071 loss_train: 1.1400 acc_train: 0.7571 loss_val: 1.1972 acc_val: 0.8100 time: 0.2055s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0072 loss_train: 1.1337 acc_train: 0.7786 loss_val: 1.1886 acc_val: 0.8100 time: 0.2114s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0073 loss_train: 1.1913 acc_train: 0.7071 loss_val: 1.1804 acc_val: 0.8133 time: 0.2565s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0074 loss_train: 1.0777 acc_train: 0.7786 loss_val: 1.1720 acc_val: 0.8133 time: 0.2468s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0075 loss_train: 1.0985 acc_train: 0.7571 loss_val: 1.1637 acc_val: 0.8167 time: 0.2349s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0076 loss_train: 1.1617 acc_train: 0.7357 loss_val: 1.1556 acc_val: 0.8167 time: 0.2218s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0077 loss_train: 1.1874 acc_train: 0.7071 loss_val: 1.1481 acc_val: 0.8167 time: 0.2107s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0078 loss_train: 1.0521 acc_train: 0.7214 loss_val: 1.1407 acc_val: 0.8167 time: 0.2094s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0079 loss_train: 1.0308 acc_train: 0.7429 loss_val: 1.1335 acc_val: 0.8233 time: 0.2095s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0080 loss_train: 1.0222 acc_train: 0.7357 loss_val: 1.1263 acc_val: 0.8300 time: 0.2082s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0081 loss_train: 1.1010 acc_train: 0.7357 loss_val: 1.1189 acc_val: 0.8233 time: 0.2076s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0082 loss_train: 1.0500 acc_train: 0.7214 loss_val: 1.1116 acc_val: 0.8233 time: 0.2638s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0083 loss_train: 0.9813 acc_train: 0.7786 loss_val: 1.1041 acc_val: 0.8233 time: 0.2452s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0084 loss_train: 1.0662 acc_train: 0.7571 loss_val: 1.0965 acc_val: 0.8233 time: 0.2343s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0085 loss_train: 1.1212 acc_train: 0.7429 loss_val: 1.0891 acc_val: 0.8233 time: 0.2222s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0086 loss_train: 1.0528 acc_train: 0.7214 loss_val: 1.0820 acc_val: 0.8233 time: 0.2111s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0087 loss_train: 1.1087 acc_train: 0.7071 loss_val: 1.0753 acc_val: 0.8233 time: 0.2208s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0088 loss_train: 0.9515 acc_train: 0.7857 loss_val: 1.0688 acc_val: 0.8267 time: 0.2136s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0089 loss_train: 1.0742 acc_train: 0.7357 loss_val: 1.0623 acc_val: 0.8300 time: 0.2094s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0090 loss_train: 1.0304 acc_train: 0.7429 loss_val: 1.0560 acc_val: 0.8300 time: 0.2103s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0091 loss_train: 0.9745 acc_train: 0.7786 loss_val: 1.0497 acc_val: 0.8300 time: 0.2682s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0092 loss_train: 0.9442 acc_train: 0.7643 loss_val: 1.0435 acc_val: 0.8300 time: 0.2508s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0093 loss_train: 0.9647 acc_train: 0.7500 loss_val: 1.0372 acc_val: 0.8300 time: 0.2402s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0094 loss_train: 1.0284 acc_train: 0.7429 loss_val: 1.0308 acc_val: 0.8333 time: 0.2396s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0095 loss_train: 1.0122 acc_train: 0.7571 loss_val: 1.0247 acc_val: 0.8333 time: 0.2359s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0096 loss_train: 0.9621 acc_train: 0.7786 loss_val: 1.0186 acc_val: 0.8333 time: 0.2268s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0097 loss_train: 0.9881 acc_train: 0.7500 loss_val: 1.0127 acc_val: 0.8333 time: 0.2165s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0098 loss_train: 0.9442 acc_train: 0.7929 loss_val: 1.0070 acc_val: 0.8333 time: 0.2072s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0099 loss_train: 0.9744 acc_train: 0.7500 loss_val: 1.0013 acc_val: 0.8367 time: 0.2046s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0100 loss_train: 0.9766 acc_train: 0.7429 loss_val: 0.9958 acc_val: 0.8433 time: 0.2580s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0101 loss_train: 0.8978 acc_train: 0.8143 loss_val: 0.9902 acc_val: 0.8433 time: 0.2457s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0102 loss_train: 0.9576 acc_train: 0.7929 loss_val: 0.9848 acc_val: 0.8433 time: 0.2293s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0103 loss_train: 0.9781 acc_train: 0.7786 loss_val: 0.9796 acc_val: 0.8400 time: 0.2177s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0104 loss_train: 0.8976 acc_train: 0.8286 loss_val: 0.9744 acc_val: 0.8400 time: 0.2087s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0105 loss_train: 1.0169 acc_train: 0.7429 loss_val: 0.9693 acc_val: 0.8400 time: 0.2066s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0106 loss_train: 0.9571 acc_train: 0.7786 loss_val: 0.9645 acc_val: 0.8400 time: 0.2119s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0107 loss_train: 0.8677 acc_train: 0.8143 loss_val: 0.9598 acc_val: 0.8400 time: 0.2069s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0108 loss_train: 0.9906 acc_train: 0.7571 loss_val: 0.9551 acc_val: 0.8367 time: 0.2068s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0109 loss_train: 0.8845 acc_train: 0.8000 loss_val: 0.9507 acc_val: 0.8367 time: 0.2114s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0110 loss_train: 0.9058 acc_train: 0.7786 loss_val: 0.9462 acc_val: 0.8367 time: 0.2646s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0111 loss_train: 1.0011 acc_train: 0.7500 loss_val: 0.9422 acc_val: 0.8367 time: 0.2465s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0112 loss_train: 0.8428 acc_train: 0.8214 loss_val: 0.9382 acc_val: 0.8333 time: 0.2303s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0113 loss_train: 0.9163 acc_train: 0.7643 loss_val: 0.9343 acc_val: 0.8333 time: 0.2243s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0114 loss_train: 0.8350 acc_train: 0.8071 loss_val: 0.9304 acc_val: 0.8333 time: 0.2089s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0115 loss_train: 0.8391 acc_train: 0.7929 loss_val: 0.9266 acc_val: 0.8333 time: 0.2116s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0116 loss_train: 0.8768 acc_train: 0.7714 loss_val: 0.9229 acc_val: 0.8333 time: 0.2330s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0117 loss_train: 0.9283 acc_train: 0.7500 loss_val: 0.9193 acc_val: 0.8300 time: 0.2285s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0118 loss_train: 0.8678 acc_train: 0.8071 loss_val: 0.9159 acc_val: 0.8300 time: 0.2679s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0119 loss_train: 0.8621 acc_train: 0.7786 loss_val: 0.9123 acc_val: 0.8300 time: 0.2450s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0120 loss_train: 0.8547 acc_train: 0.7929 loss_val: 0.9087 acc_val: 0.8300 time: 0.2338s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0121 loss_train: 0.9430 acc_train: 0.7357 loss_val: 0.9051 acc_val: 0.8300 time: 0.2259s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0122 loss_train: 0.9013 acc_train: 0.7786 loss_val: 0.9016 acc_val: 0.8300 time: 0.2117s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0123 loss_train: 0.8996 acc_train: 0.7929 loss_val: 0.8983 acc_val: 0.8300 time: 0.2173s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0124 loss_train: 0.8529 acc_train: 0.7857 loss_val: 0.8949 acc_val: 0.8333 time: 0.2079s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0125 loss_train: 0.8443 acc_train: 0.8000 loss_val: 0.8916 acc_val: 0.8333 time: 0.2172s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0126 loss_train: 0.7669 acc_train: 0.8500 loss_val: 0.8880 acc_val: 0.8333 time: 0.2087s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0127 loss_train: 0.9035 acc_train: 0.7714 loss_val: 0.8845 acc_val: 0.8333 time: 0.2583s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0128 loss_train: 0.8324 acc_train: 0.8286 loss_val: 0.8809 acc_val: 0.8333 time: 0.2453s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0129 loss_train: 0.8808 acc_train: 0.7786 loss_val: 0.8778 acc_val: 0.8333 time: 0.2357s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0130 loss_train: 0.8187 acc_train: 0.8143 loss_val: 0.8746 acc_val: 0.8333 time: 0.2207s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0131 loss_train: 0.8769 acc_train: 0.8214 loss_val: 0.8714 acc_val: 0.8333 time: 0.2188s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0132 loss_train: 0.8372 acc_train: 0.8000 loss_val: 0.8682 acc_val: 0.8333 time: 0.2109s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0133 loss_train: 0.7891 acc_train: 0.8214 loss_val: 0.8650 acc_val: 0.8333 time: 0.2089s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0134 loss_train: 0.8286 acc_train: 0.8286 loss_val: 0.8618 acc_val: 0.8333 time: 0.2101s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0135 loss_train: 0.8180 acc_train: 0.8286 loss_val: 0.8585 acc_val: 0.8333 time: 0.2433s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0136 loss_train: 0.8944 acc_train: 0.7857 loss_val: 0.8553 acc_val: 0.8333 time: 0.2350s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0137 loss_train: 0.8192 acc_train: 0.7857 loss_val: 0.8523 acc_val: 0.8300 time: 0.2426s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0138 loss_train: 0.8125 acc_train: 0.7571 loss_val: 0.8491 acc_val: 0.8300 time: 0.2302s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0139 loss_train: 0.6992 acc_train: 0.8286 loss_val: 0.8458 acc_val: 0.8300 time: 0.2192s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0140 loss_train: 0.7858 acc_train: 0.8000 loss_val: 0.8426 acc_val: 0.8267 time: 0.2200s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0141 loss_train: 0.8238 acc_train: 0.8286 loss_val: 0.8394 acc_val: 0.8267 time: 0.2093s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0142 loss_train: 0.8971 acc_train: 0.8071 loss_val: 0.8362 acc_val: 0.8267 time: 0.2147s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0143 loss_train: 0.8540 acc_train: 0.7786 loss_val: 0.8332 acc_val: 0.8267 time: 0.2204s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0144 loss_train: 0.8289 acc_train: 0.7571 loss_val: 0.8303 acc_val: 0.8267 time: 0.2563s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0145 loss_train: 0.7598 acc_train: 0.8357 loss_val: 0.8275 acc_val: 0.8267 time: 0.2512s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0146 loss_train: 0.8096 acc_train: 0.7786 loss_val: 0.8249 acc_val: 0.8267 time: 0.2411s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0147 loss_train: 0.8543 acc_train: 0.8000 loss_val: 0.8224 acc_val: 0.8300 time: 0.2302s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0148 loss_train: 0.7431 acc_train: 0.8357 loss_val: 0.8198 acc_val: 0.8300 time: 0.2153s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0149 loss_train: 0.7507 acc_train: 0.7929 loss_val: 0.8174 acc_val: 0.8300 time: 0.2176s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0150 loss_train: 0.8072 acc_train: 0.7857 loss_val: 0.8148 acc_val: 0.8300 time: 0.2219s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0151 loss_train: 0.7722 acc_train: 0.8143 loss_val: 0.8126 acc_val: 0.8333 time: 0.2130s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0152 loss_train: 0.8312 acc_train: 0.8000 loss_val: 0.8105 acc_val: 0.8333 time: 0.2074s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0153 loss_train: 0.7924 acc_train: 0.7286 loss_val: 0.8086 acc_val: 0.8333 time: 0.2079s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0154 loss_train: 0.7963 acc_train: 0.8071 loss_val: 0.8069 acc_val: 0.8333 time: 0.2594s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0155 loss_train: 0.8899 acc_train: 0.7286 loss_val: 0.8054 acc_val: 0.8333 time: 0.2407s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0156 loss_train: 0.7765 acc_train: 0.7786 loss_val: 0.8039 acc_val: 0.8267 time: 0.2372s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0157 loss_train: 0.7843 acc_train: 0.8071 loss_val: 0.8023 acc_val: 0.8267 time: 0.2268s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0158 loss_train: 0.7282 acc_train: 0.8429 loss_val: 0.8006 acc_val: 0.8267 time: 0.2143s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0159 loss_train: 0.8582 acc_train: 0.7429 loss_val: 0.7989 acc_val: 0.8267 time: 0.2075s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0160 loss_train: 0.8428 acc_train: 0.7643 loss_val: 0.7974 acc_val: 0.8300 time: 0.2060s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0161 loss_train: 0.7750 acc_train: 0.7857 loss_val: 0.7960 acc_val: 0.8300 time: 0.2086s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0162 loss_train: 0.8240 acc_train: 0.8000 loss_val: 0.7946 acc_val: 0.8300 time: 0.2074s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0163 loss_train: 0.7672 acc_train: 0.7643 loss_val: 0.7933 acc_val: 0.8300 time: 0.2758s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0164 loss_train: 0.8677 acc_train: 0.7571 loss_val: 0.7917 acc_val: 0.8300 time: 0.2422s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0165 loss_train: 0.8144 acc_train: 0.7786 loss_val: 0.7904 acc_val: 0.8300 time: 0.2330s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0166 loss_train: 0.7166 acc_train: 0.8500 loss_val: 0.7890 acc_val: 0.8300 time: 0.2269s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0167 loss_train: 0.7690 acc_train: 0.8000 loss_val: 0.7878 acc_val: 0.8300 time: 0.2182s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0168 loss_train: 0.7025 acc_train: 0.8714 loss_val: 0.7865 acc_val: 0.8300 time: 0.2120s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0169 loss_train: 0.7151 acc_train: 0.8214 loss_val: 0.7854 acc_val: 0.8300 time: 0.2145s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0170 loss_train: 0.7861 acc_train: 0.7857 loss_val: 0.7843 acc_val: 0.8300 time: 0.2087s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0171 loss_train: 0.7500 acc_train: 0.8214 loss_val: 0.7830 acc_val: 0.8267 time: 0.2128s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0172 loss_train: 0.7532 acc_train: 0.7929 loss_val: 0.7818 acc_val: 0.8267 time: 0.2549s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0173 loss_train: 0.7976 acc_train: 0.7429 loss_val: 0.7809 acc_val: 0.8267 time: 0.2539s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0174 loss_train: 0.6303 acc_train: 0.8786 loss_val: 0.7800 acc_val: 0.8267 time: 0.2361s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0175 loss_train: 0.8031 acc_train: 0.7786 loss_val: 0.7791 acc_val: 0.8267 time: 0.2284s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0176 loss_train: 0.8075 acc_train: 0.7357 loss_val: 0.7783 acc_val: 0.8267 time: 0.2285s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0177 loss_train: 0.7617 acc_train: 0.8286 loss_val: 0.7775 acc_val: 0.8267 time: 0.2174s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0178 loss_train: 0.6782 acc_train: 0.8286 loss_val: 0.7767 acc_val: 0.8267 time: 0.2135s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0179 loss_train: 0.7301 acc_train: 0.8143 loss_val: 0.7758 acc_val: 0.8267 time: 0.2097s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0180 loss_train: 0.7284 acc_train: 0.8214 loss_val: 0.7746 acc_val: 0.8267 time: 0.2123s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0181 loss_train: 0.7863 acc_train: 0.7857 loss_val: 0.7731 acc_val: 0.8233 time: 0.2097s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0182 loss_train: 0.7274 acc_train: 0.8357 loss_val: 0.7716 acc_val: 0.8233 time: 0.2514s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0183 loss_train: 0.7704 acc_train: 0.8000 loss_val: 0.7701 acc_val: 0.8200 time: 0.2369s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0184 loss_train: 0.7739 acc_train: 0.7929 loss_val: 0.7686 acc_val: 0.8167 time: 0.2298s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0185 loss_train: 0.7872 acc_train: 0.7929 loss_val: 0.7671 acc_val: 0.8167 time: 0.2190s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0186 loss_train: 0.8271 acc_train: 0.7571 loss_val: 0.7657 acc_val: 0.8133 time: 0.2083s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0187 loss_train: 0.8094 acc_train: 0.7714 loss_val: 0.7646 acc_val: 0.8133 time: 0.2104s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0188 loss_train: 0.7929 acc_train: 0.7857 loss_val: 0.7636 acc_val: 0.8100 time: 0.2051s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0189 loss_train: 0.8361 acc_train: 0.7571 loss_val: 0.7629 acc_val: 0.8100 time: 0.2048s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0190 loss_train: 0.6002 acc_train: 0.8929 loss_val: 0.7620 acc_val: 0.8100 time: 0.2058s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0191 loss_train: 0.7376 acc_train: 0.8357 loss_val: 0.7607 acc_val: 0.8100 time: 0.2563s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0192 loss_train: 0.7822 acc_train: 0.8143 loss_val: 0.7594 acc_val: 0.8100 time: 0.2417s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0193 loss_train: 0.6686 acc_train: 0.8357 loss_val: 0.7581 acc_val: 0.8100 time: 0.2354s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0194 loss_train: 0.8100 acc_train: 0.7786 loss_val: 0.7567 acc_val: 0.8100 time: 0.2194s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0195 loss_train: 0.7760 acc_train: 0.8286 loss_val: 0.7552 acc_val: 0.8100 time: 0.2169s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0196 loss_train: 0.7589 acc_train: 0.7929 loss_val: 0.7538 acc_val: 0.8100 time: 0.2069s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0197 loss_train: 0.7159 acc_train: 0.8357 loss_val: 0.7526 acc_val: 0.8133 time: 0.2081s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0198 loss_train: 0.7465 acc_train: 0.8143 loss_val: 0.7512 acc_val: 0.8133 time: 0.2072s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0199 loss_train: 0.7724 acc_train: 0.8071 loss_val: 0.7498 acc_val: 0.8133 time: 0.2054s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0200 loss_train: 0.7318 acc_train: 0.8143 loss_val: 0.7482 acc_val: 0.8133 time: 0.2332s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0201 loss_train: 0.6718 acc_train: 0.8429 loss_val: 0.7466 acc_val: 0.8100 time: 0.2783s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0202 loss_train: 0.6702 acc_train: 0.7714 loss_val: 0.7453 acc_val: 0.8100 time: 0.2447s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0203 loss_train: 0.6907 acc_train: 0.8286 loss_val: 0.7439 acc_val: 0.8100 time: 0.2293s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0204 loss_train: 0.7721 acc_train: 0.7571 loss_val: 0.7428 acc_val: 0.8100 time: 0.2244s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0205 loss_train: 0.7214 acc_train: 0.8214 loss_val: 0.7416 acc_val: 0.8100 time: 0.2116s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0206 loss_train: 0.7251 acc_train: 0.8000 loss_val: 0.7404 acc_val: 0.8100 time: 0.2180s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0207 loss_train: 0.7419 acc_train: 0.7929 loss_val: 0.7393 acc_val: 0.8100 time: 0.2109s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0208 loss_train: 0.7670 acc_train: 0.8000 loss_val: 0.7380 acc_val: 0.8100 time: 0.2090s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0209 loss_train: 0.7406 acc_train: 0.8000 loss_val: 0.7366 acc_val: 0.8100 time: 0.2525s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0210 loss_train: 0.7487 acc_train: 0.7929 loss_val: 0.7354 acc_val: 0.8100 time: 0.2423s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0211 loss_train: 0.7354 acc_train: 0.7857 loss_val: 0.7343 acc_val: 0.8100 time: 0.2303s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0212 loss_train: 0.7288 acc_train: 0.8071 loss_val: 0.7333 acc_val: 0.8100 time: 0.2196s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0213 loss_train: 0.7886 acc_train: 0.7929 loss_val: 0.7324 acc_val: 0.8100 time: 0.2191s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0214 loss_train: 0.7986 acc_train: 0.8000 loss_val: 0.7315 acc_val: 0.8100 time: 0.2144s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0215 loss_train: 0.7278 acc_train: 0.8143 loss_val: 0.7307 acc_val: 0.8133 time: 0.2075s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0216 loss_train: 0.7075 acc_train: 0.7929 loss_val: 0.7301 acc_val: 0.8133 time: 0.2079s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0217 loss_train: 0.7518 acc_train: 0.7643 loss_val: 0.7296 acc_val: 0.8167 time: 0.2574s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0218 loss_train: 0.6739 acc_train: 0.8571 loss_val: 0.7289 acc_val: 0.8167 time: 0.2459s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0219 loss_train: 0.6932 acc_train: 0.7929 loss_val: 0.7285 acc_val: 0.8167 time: 0.2274s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0220 loss_train: 0.6494 acc_train: 0.8429 loss_val: 0.7283 acc_val: 0.8133 time: 0.2298s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0221 loss_train: 0.6965 acc_train: 0.8214 loss_val: 0.7279 acc_val: 0.8133 time: 0.2190s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0222 loss_train: 0.7560 acc_train: 0.8071 loss_val: 0.7275 acc_val: 0.8133 time: 0.2132s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0223 loss_train: 0.7351 acc_train: 0.7714 loss_val: 0.7268 acc_val: 0.8133 time: 0.2113s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0224 loss_train: 0.7181 acc_train: 0.8571 loss_val: 0.7260 acc_val: 0.8133 time: 0.2141s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0225 loss_train: 0.7134 acc_train: 0.8071 loss_val: 0.7253 acc_val: 0.8133 time: 0.2150s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0226 loss_train: 0.6928 acc_train: 0.8286 loss_val: 0.7247 acc_val: 0.8133 time: 0.2638s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0227 loss_train: 0.6539 acc_train: 0.8286 loss_val: 0.7242 acc_val: 0.8167 time: 0.2454s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0228 loss_train: 0.6190 acc_train: 0.8071 loss_val: 0.7236 acc_val: 0.8167 time: 0.2274s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0229 loss_train: 0.7450 acc_train: 0.7857 loss_val: 0.7232 acc_val: 0.8167 time: 0.2288s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0230 loss_train: 0.6649 acc_train: 0.8000 loss_val: 0.7229 acc_val: 0.8167 time: 0.2159s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0231 loss_train: 0.6978 acc_train: 0.8429 loss_val: 0.7226 acc_val: 0.8167 time: 0.2078s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0232 loss_train: 0.7310 acc_train: 0.8143 loss_val: 0.7222 acc_val: 0.8167 time: 0.2105s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0233 loss_train: 0.6790 acc_train: 0.8000 loss_val: 0.7218 acc_val: 0.8167 time: 0.2084s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0234 loss_train: 0.7864 acc_train: 0.7857 loss_val: 0.7215 acc_val: 0.8167 time: 0.2036s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0235 loss_train: 0.6192 acc_train: 0.8429 loss_val: 0.7213 acc_val: 0.8167 time: 0.2546s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0236 loss_train: 0.7390 acc_train: 0.7500 loss_val: 0.7212 acc_val: 0.8167 time: 0.2382s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0237 loss_train: 0.6972 acc_train: 0.8000 loss_val: 0.7212 acc_val: 0.8167 time: 0.2316s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0238 loss_train: 0.6559 acc_train: 0.8286 loss_val: 0.7211 acc_val: 0.8167 time: 0.2193s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0239 loss_train: 0.7856 acc_train: 0.8143 loss_val: 0.7207 acc_val: 0.8167 time: 0.2101s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0240 loss_train: 0.6656 acc_train: 0.8214 loss_val: 0.7203 acc_val: 0.8167 time: 0.2055s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0241 loss_train: 0.6982 acc_train: 0.8286 loss_val: 0.7201 acc_val: 0.8167 time: 0.2069s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0242 loss_train: 0.7340 acc_train: 0.7714 loss_val: 0.7193 acc_val: 0.8167 time: 0.2060s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0243 loss_train: 0.7024 acc_train: 0.8571 loss_val: 0.7184 acc_val: 0.8167 time: 0.2086s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0244 loss_train: 0.6702 acc_train: 0.8286 loss_val: 0.7175 acc_val: 0.8167 time: 0.2075s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0245 loss_train: 0.7385 acc_train: 0.7929 loss_val: 0.7166 acc_val: 0.8133 time: 0.2572s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0246 loss_train: 0.6413 acc_train: 0.8643 loss_val: 0.7157 acc_val: 0.8133 time: 0.2398s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0247 loss_train: 0.8283 acc_train: 0.7714 loss_val: 0.7147 acc_val: 0.8133 time: 0.2193s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0248 loss_train: 0.6448 acc_train: 0.8643 loss_val: 0.7136 acc_val: 0.8200 time: 0.2449s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0249 loss_train: 0.7209 acc_train: 0.7929 loss_val: 0.7129 acc_val: 0.8200 time: 0.2283s\n",
            "Loading cora datasetss...\n",
            "Epoch: 0250 loss_train: 0.7495 acc_train: 0.7643 loss_val: 0.7124 acc_val: 0.8200 time: 0.2053s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1308.5817s\n",
            "Loading 249th epoch\n",
            "Test set results: loss= 0.7448 accuracy= 0.8380\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:115: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGZFo9lULm39",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8de14d96-fcce-4776-8ef8-ca4e349775c9"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from utils import load_data, accuracy\n",
        "from models import GAT, SpGAT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_test(noise_level):\n",
        "\n",
        "    acc = []\n",
        "    loss = []\n",
        "\n",
        "    for n in noise_level:\n",
        "        print(n)\n",
        "        # Load data with noise\n",
        "        print('now adding eps = ', n)\n",
        "        adj, features, labels, idx_train, idx_val, idx_test = load_data(noise_level_test = n)\n",
        "        \n",
        "        model.cuda()\n",
        "        features = features.cuda()\n",
        "        adj = adj.cuda()\n",
        "        labels = labels.cuda()\n",
        "        idx_train = idx_train.cuda()\n",
        "        idx_val = idx_val.cuda()\n",
        "        idx_test = idx_test.cuda()\n",
        "            \n",
        "        features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
        "\n",
        "        # model = SpGAT(nfeat=features.shape[1], \n",
        "        #               nhid=8, \n",
        "        #               nclass=int(labels.max()) + 1, \n",
        "        #               dropout=0.6, \n",
        "        #               nheads=8, \n",
        "        #               alpha=0.2)\n",
        "\n",
        "\n",
        "\n",
        "        # Restore best model\n",
        "        best_epoch = 249\n",
        "        print('Loading {}th epoch'.format(best_epoch))\n",
        "        model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        #output = add_noise(output, eps=0.1)\n",
        "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "        print(\"Test set results for eps = \", n, ' : ',\n",
        "              \"loss= {:.4f}\".format(loss_test.data[0]),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.data[0]))\n",
        "        \n",
        "        loss.append(loss_test.data[0])\n",
        "        acc.append(acc_test.data[0])\n",
        "\n",
        "    return loss,acc\n",
        "    \n",
        "\n",
        "# Testing\n",
        "\n",
        "noise_level = np.linspace(0, 0.9, num=20)\n",
        "\n",
        "loss,acc = compute_test(noise_level)\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "now adding eps =  0.0\n",
            "Loading cora datasetss...\n",
            "Loading 249th epoch\n",
            "Test set results for eps =  0.0  :  loss= 0.7448 accuracy= 0.8380\n",
            "0.04736842105263158\n",
            "now adding eps =  0.04736842105263158\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.04736842105263158  :  loss= 1.1961 accuracy= 0.7800\n",
            "0.09473684210526316\n",
            "now adding eps =  0.09473684210526316\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.09473684210526316  :  loss= 1.2697 accuracy= 0.7480\n",
            "0.14210526315789473\n",
            "now adding eps =  0.14210526315789473\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.14210526315789473  :  loss= 1.2972 accuracy= 0.7450\n",
            "0.18947368421052632\n",
            "now adding eps =  0.18947368421052632\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.18947368421052632  :  loss= 1.3161 accuracy= 0.7430\n",
            "0.2368421052631579\n",
            "now adding eps =  0.2368421052631579\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.2368421052631579  :  loss= 1.3243 accuracy= 0.7280\n",
            "0.28421052631578947\n",
            "now adding eps =  0.28421052631578947\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.28421052631578947  :  loss= 1.3318 accuracy= 0.7290\n",
            "0.3315789473684211\n",
            "now adding eps =  0.3315789473684211\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.3315789473684211  :  loss= 1.3371 accuracy= 0.7250\n",
            "0.37894736842105264\n",
            "now adding eps =  0.37894736842105264\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.37894736842105264  :  loss= 1.3432 accuracy= 0.7180\n",
            "0.4263157894736842\n",
            "now adding eps =  0.4263157894736842\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.4263157894736842  :  loss= 1.3470 accuracy= 0.7210\n",
            "0.4736842105263158\n",
            "now adding eps =  0.4736842105263158\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.4736842105263158  :  loss= 1.3470 accuracy= 0.7230\n",
            "0.5210526315789474\n",
            "now adding eps =  0.5210526315789474\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.5210526315789474  :  loss= 1.3494 accuracy= 0.7200\n",
            "0.5684210526315789\n",
            "now adding eps =  0.5684210526315789\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.5684210526315789  :  loss= 1.3512 accuracy= 0.7190\n",
            "0.6157894736842106\n",
            "now adding eps =  0.6157894736842106\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.6157894736842106  :  loss= 1.3536 accuracy= 0.7140\n",
            "0.6631578947368422\n",
            "now adding eps =  0.6631578947368422\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.6631578947368422  :  loss= 1.3554 accuracy= 0.7160\n",
            "0.7105263157894737\n",
            "now adding eps =  0.7105263157894737\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.7105263157894737  :  loss= 1.3552 accuracy= 0.7160\n",
            "0.7578947368421053\n",
            "now adding eps =  0.7578947368421053\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.7578947368421053  :  loss= 1.3567 accuracy= 0.7170\n",
            "0.8052631578947369\n",
            "now adding eps =  0.8052631578947369\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.8052631578947369  :  loss= 1.3574 accuracy= 0.7160\n",
            "0.8526315789473684\n",
            "now adding eps =  0.8526315789473684\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.8526315789473684  :  loss= 1.3578 accuracy= 0.7130\n",
            "0.9\n",
            "now adding eps =  0.9\n",
            "Loading cora datasetss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 249th epoch\n",
            "Test set results for eps =  0.9  :  loss= 1.3590 accuracy= 0.7140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb8C940bNzY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d20fdf46-4b4f-4ea9-9a96-3cef16662fc9"
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.plot(noise_level, acc, 'g')\n",
        "#plt.xscale('log')\n",
        "format_plot(x=r'$\\epsilon$', y='accuracy')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(noise_level, loss, 'r')\n",
        "\n",
        "format_plot(x=r'$\\epsilon$', y='loss')\n",
        "finalize_plot(shape=(1.2, 0.6))\n",
        "plt.savefig('acc-loss-trained-with-noise.pdf')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/pdf": "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\n",
            "text/plain": [
              "<Figure size 518.4x259.2 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"246.939687pt\" version=\"1.1\" viewBox=\"0 0 505.485937 246.939687\" width=\"505.485937pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 246.939687 \nL 505.485937 246.939687 \nL 505.485937 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.805937 203.98 \nL 249.003437 203.98 \nL 249.003437 7.2 \nL 56.805937 7.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 65.542187 203.98 \nL 65.542187 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 3.421875 \nQ 39.265625 3.421875 42.96875 11.625 \nQ 46.6875 19.828125 46.6875 36.375 \nQ 46.6875 52.984375 42.96875 61.1875 \nQ 39.265625 69.390625 31.78125 69.390625 \nQ 24.3125 69.390625 20.59375 61.1875 \nQ 16.890625 52.984375 16.890625 36.375 \nQ 16.890625 19.828125 20.59375 11.625 \nQ 24.3125 3.421875 31.78125 3.421875 \nz\nM 31.78125 -1.421875 \nQ 19.921875 -1.421875 13.25 8.53125 \nQ 6.59375 18.5 6.59375 36.375 \nQ 6.59375 54.296875 13.25 64.25 \nQ 19.921875 74.21875 31.78125 74.21875 \nQ 43.703125 74.21875 50.34375 64.25 \nQ 56.984375 54.296875 56.984375 36.375 \nQ 56.984375 18.5 50.34375 8.53125 \nQ 43.703125 -1.421875 31.78125 -1.421875 \nz\n\" id=\"DejaVuSerif-48\"/>\n       <path d=\"M 9.421875 5.078125 \nQ 9.421875 7.8125 11.28125 9.71875 \nQ 13.140625 11.625 15.921875 11.625 \nQ 18.609375 11.625 20.5 9.71875 \nQ 22.40625 7.8125 22.40625 5.078125 \nQ 22.40625 2.390625 20.5 0.484375 \nQ 18.609375 -1.421875 15.921875 -1.421875 \nQ 13.140625 -1.421875 11.28125 0.453125 \nQ 9.421875 2.34375 9.421875 5.078125 \nz\n\" id=\"DejaVuSerif-46\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(56.795469 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 104.369965 203.98 \nL 104.369965 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 12.796875 55.515625 \nL 7.328125 55.515625 \nL 7.328125 68.5 \nQ 12.546875 71.296875 17.84375 72.75 \nQ 23.140625 74.21875 28.21875 74.21875 \nQ 39.59375 74.21875 46.1875 68.703125 \nQ 52.78125 63.1875 52.78125 53.71875 \nQ 52.78125 43.015625 37.84375 28.125 \nQ 36.671875 27 36.078125 26.421875 \nL 17.671875 8.015625 \nL 48.09375 8.015625 \nL 48.09375 17 \nL 53.8125 17 \nL 53.8125 0 \nL 6.78125 0 \nL 6.78125 5.328125 \nL 28.90625 27.390625 \nQ 36.234375 34.71875 39.359375 40.84375 \nQ 42.484375 46.96875 42.484375 53.71875 \nQ 42.484375 61.078125 38.640625 65.234375 \nQ 34.8125 69.390625 28.078125 69.390625 \nQ 21.09375 69.390625 17.28125 65.921875 \nQ 13.484375 62.453125 12.796875 55.515625 \nz\n\" id=\"DejaVuSerif-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(95.623247 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 143.197743 203.98 \nL 143.197743 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 34.90625 24.703125 \nL 34.90625 63.484375 \nL 10.015625 24.703125 \nz\nM 56.390625 0 \nL 23.1875 0 \nL 23.1875 5.171875 \nL 34.90625 5.171875 \nL 34.90625 19.484375 \nL 3.078125 19.484375 \nL 3.078125 24.8125 \nL 35.015625 74.21875 \nL 44.671875 74.21875 \nL 44.671875 24.703125 \nL 58.59375 24.703125 \nL 58.59375 19.484375 \nL 44.671875 19.484375 \nL 44.671875 5.171875 \nL 56.390625 5.171875 \nz\n\" id=\"DejaVuSerif-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(134.451024 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 182.025521 203.98 \nL 182.025521 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 32.71875 3.421875 \nQ 39.59375 3.421875 43.296875 8.46875 \nQ 47.015625 13.53125 47.015625 23 \nQ 47.015625 32.46875 43.296875 37.515625 \nQ 39.59375 42.578125 32.71875 42.578125 \nQ 25.734375 42.578125 22.0625 37.6875 \nQ 18.40625 32.8125 18.40625 23.578125 \nQ 18.40625 13.875 22.109375 8.640625 \nQ 25.828125 3.421875 32.71875 3.421875 \nz\nM 16.796875 40.140625 \nQ 20.125 43.796875 24.3125 45.59375 \nQ 28.515625 47.40625 33.796875 47.40625 \nQ 44.671875 47.40625 51 40.859375 \nQ 57.328125 34.328125 57.328125 23 \nQ 57.328125 11.921875 50.515625 5.25 \nQ 43.703125 -1.421875 32.328125 -1.421875 \nQ 19.96875 -1.421875 13.328125 7.78125 \nQ 6.6875 17 6.6875 34.078125 \nQ 6.6875 53.21875 14.546875 63.71875 \nQ 22.40625 74.21875 36.71875 74.21875 \nQ 40.578125 74.21875 44.828125 73.484375 \nQ 49.078125 72.75 53.515625 71.296875 \nL 53.515625 59.28125 \nL 48 59.28125 \nQ 47.40625 64.203125 44.234375 66.796875 \nQ 41.0625 69.390625 35.6875 69.390625 \nQ 26.21875 69.390625 21.578125 62.203125 \nQ 16.9375 55.03125 16.796875 40.140625 \nz\n\" id=\"DejaVuSerif-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(173.278802 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 220.853299 203.98 \nL 220.853299 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 46.578125 19.921875 \nQ 46.578125 27.734375 42.6875 32.046875 \nQ 38.8125 36.375 31.78125 36.375 \nQ 24.75 36.375 20.875 32.046875 \nQ 17 27.734375 17 19.921875 \nQ 17 12.0625 20.875 7.734375 \nQ 24.75 3.421875 31.78125 3.421875 \nQ 38.8125 3.421875 42.6875 7.734375 \nQ 46.578125 12.0625 46.578125 19.921875 \nz\nM 44.578125 55.328125 \nQ 44.578125 61.96875 41.203125 65.671875 \nQ 37.84375 69.390625 31.78125 69.390625 \nQ 25.78125 69.390625 22.390625 65.671875 \nQ 19 61.96875 19 55.328125 \nQ 19 48.640625 22.390625 44.921875 \nQ 25.78125 41.21875 31.78125 41.21875 \nQ 37.84375 41.21875 41.203125 44.921875 \nQ 44.578125 48.640625 44.578125 55.328125 \nz\nM 39.3125 38.8125 \nQ 47.609375 37.703125 52.25 32.6875 \nQ 56.890625 27.6875 56.890625 19.921875 \nQ 56.890625 9.671875 50.390625 4.125 \nQ 43.890625 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.203125 4.125 \nQ 6.6875 9.671875 6.6875 19.921875 \nQ 6.6875 27.6875 11.328125 32.6875 \nQ 15.96875 37.703125 24.3125 38.8125 \nQ 16.9375 40.140625 13 44.40625 \nQ 9.078125 48.6875 9.078125 55.328125 \nQ 9.078125 64.109375 15.125 69.15625 \nQ 21.1875 74.21875 31.78125 74.21875 \nQ 42.390625 74.21875 48.4375 69.15625 \nQ 54.5 64.109375 54.5 55.328125 \nQ 54.5 48.6875 50.5625 44.40625 \nQ 46.625 40.140625 39.3125 38.8125 \nz\n\" id=\"DejaVuSerif-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(212.10658 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $\\epsilon$ -->\n     <defs>\n      <path d=\"M 19.734375 29.5 \nQ 14.453125 30.671875 12.15625 33.84375 \nQ 10.453125 36.078125 10.453125 39.109375 \nQ 10.453125 47.40625 18.359375 52.25 \nQ 24.609375 56.0625 34.1875 56.0625 \nQ 37.890625 56.0625 41.9375 55.46875 \nQ 46 54.890625 50.53125 53.71875 \nL 48.96875 45.5625 \nQ 44.484375 46.96875 40.71875 47.609375 \nQ 36.859375 48.25 33.40625 48.25 \nQ 27.59375 48.25 23.78125 46 \nQ 19.1875 43.3125 19.1875 39.40625 \nQ 19.1875 36.8125 21.578125 35.015625 \nQ 24.421875 32.859375 30.078125 32.859375 \nL 37.640625 32.859375 \nL 36.234375 25.4375 \nL 29 25.4375 \nQ 22.265625 25.4375 18.3125 22.953125 \nQ 12.9375 19.578125 12.9375 14.3125 \nQ 12.9375 10.984375 15.828125 8.796875 \nQ 19.4375 6.0625 26.8125 6.0625 \nQ 31.34375 6.0625 35.6875 6.9375 \nQ 40.046875 7.859375 43.84375 9.671875 \nL 42.1875 1.3125 \nQ 37.546875 -0.046875 33.296875 -0.734375 \nQ 29.046875 -1.421875 25.140625 -1.421875 \nQ 13.53125 -1.421875 8.0625 3.03125 \nQ 3.90625 6.453125 3.90625 12.203125 \nQ 3.90625 19.96875 9.375 24.75 \nQ 13.421875 28.328125 19.734375 29.5 \nz\n\" id=\"DejaVuSans-Oblique-949\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(149.604687 237.244062)scale(0.12 -0.12)\">\n      <use transform=\"translate(0 0.9375)\" xlink:href=\"#DejaVuSans-Oblique-949\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 56.805937 185.017564 \nL 249.003437 185.017564 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.72 -->\n      <defs>\n       <path d=\"M 56.390625 67.921875 \nL 27.875 0 \nL 20.609375 0 \nL 47.796875 64.890625 \nL 14.109375 64.890625 \nL 14.109375 55.90625 \nL 8.40625 55.90625 \nL 8.40625 72.90625 \nL 56.390625 72.90625 \nz\n\" id=\"DejaVuSerif-55\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 189.196704)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 56.805937 156.395018 \nL 249.003437 156.395018 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.74 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 160.574159)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 56.805937 127.772473 \nL 249.003437 127.772473 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.76 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 131.951613)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 56.805937 99.149927 \nL 249.003437 99.149927 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.78 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 103.329068)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 56.805937 70.527382 \nL 249.003437 70.527382 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.80 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 74.706522)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 56.805937 41.904836 \nL 249.003437 41.904836 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.82 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 46.083977)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p2079331ca8)\" d=\"M 56.805937 13.282291 \nL 249.003437 13.282291 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.84 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 17.461432)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- accuracy -->\n     <defs>\n      <path d=\"M 39.796875 16.3125 \nL 39.796875 27.296875 \nL 28.21875 27.296875 \nQ 21.53125 27.296875 18.25 24.40625 \nQ 14.984375 21.53125 14.984375 15.578125 \nQ 14.984375 10.15625 18.296875 6.984375 \nQ 21.625 3.8125 27.296875 3.8125 \nQ 32.90625 3.8125 36.34375 7.28125 \nQ 39.796875 10.75 39.796875 16.3125 \nz\nM 48.78125 32.421875 \nL 48.78125 5.171875 \nL 56.78125 5.171875 \nL 56.78125 0 \nL 39.796875 0 \nL 39.796875 5.609375 \nQ 36.8125 2 32.90625 0.28125 \nQ 29 -1.421875 23.78125 -1.421875 \nQ 15.140625 -1.421875 10.0625 3.171875 \nQ 4.984375 7.765625 4.984375 15.578125 \nQ 4.984375 23.640625 10.796875 28.078125 \nQ 16.609375 32.515625 27.203125 32.515625 \nL 39.796875 32.515625 \nL 39.796875 36.078125 \nQ 39.796875 42 36.203125 45.234375 \nQ 32.625 48.484375 26.125 48.484375 \nQ 20.75 48.484375 17.578125 46.046875 \nQ 14.40625 43.609375 13.625 38.8125 \nL 8.984375 38.8125 \nL 8.984375 49.3125 \nQ 13.671875 51.3125 18.09375 52.3125 \nQ 22.515625 53.328125 26.703125 53.328125 \nQ 37.5 53.328125 43.140625 47.96875 \nQ 48.78125 42.625 48.78125 32.421875 \nz\n\" id=\"DejaVuSerif-97\"/>\n      <path d=\"M 51.421875 15.578125 \nQ 49.515625 7.28125 44.09375 2.921875 \nQ 38.671875 -1.421875 30.078125 -1.421875 \nQ 18.75 -1.421875 11.859375 6.078125 \nQ 4.984375 13.578125 4.984375 25.984375 \nQ 4.984375 38.421875 11.859375 45.875 \nQ 18.75 53.328125 30.078125 53.328125 \nQ 35.015625 53.328125 39.890625 52.171875 \nQ 44.78125 51.03125 49.703125 48.6875 \nL 49.703125 35.40625 \nL 44.484375 35.40625 \nQ 43.453125 42.234375 40.015625 45.359375 \nQ 36.578125 48.484375 30.171875 48.484375 \nQ 22.90625 48.484375 19.1875 42.84375 \nQ 15.484375 37.203125 15.484375 25.984375 \nQ 15.484375 14.75 19.171875 9.078125 \nQ 22.859375 3.421875 30.171875 3.421875 \nQ 35.984375 3.421875 39.453125 6.4375 \nQ 42.921875 9.46875 44.1875 15.578125 \nz\n\" id=\"DejaVuSerif-99\"/>\n      <path d=\"M 35.40625 51.90625 \nL 52.203125 51.90625 \nL 52.203125 5.171875 \nL 60.6875 5.171875 \nL 60.6875 0 \nL 43.21875 0 \nL 43.21875 9.1875 \nQ 40.71875 4 36.765625 1.28125 \nQ 32.8125 -1.421875 27.59375 -1.421875 \nQ 18.953125 -1.421875 14.875 3.484375 \nQ 10.796875 8.40625 10.796875 18.890625 \nL 10.796875 46.6875 \nL 2.6875 46.6875 \nL 2.6875 51.90625 \nL 19.828125 51.90625 \nL 19.828125 21.6875 \nQ 19.828125 12.203125 22.140625 8.6875 \nQ 24.46875 5.171875 30.421875 5.171875 \nQ 36.671875 5.171875 39.9375 9.765625 \nQ 43.21875 14.359375 43.21875 23.09375 \nL 43.21875 46.6875 \nL 35.40625 46.6875 \nz\n\" id=\"DejaVuSerif-117\"/>\n      <path d=\"M 47.796875 52 \nL 47.796875 39.015625 \nL 42.625 39.015625 \nQ 42.390625 42.875 40.484375 44.78125 \nQ 38.578125 46.6875 34.90625 46.6875 \nQ 28.265625 46.6875 24.71875 42.09375 \nQ 21.1875 37.5 21.1875 28.90625 \nL 21.1875 5.171875 \nL 31.59375 5.171875 \nL 31.59375 0 \nL 4.109375 0 \nL 4.109375 5.171875 \nL 12.203125 5.171875 \nL 12.203125 46.78125 \nL 3.609375 46.78125 \nL 3.609375 51.90625 \nL 21.1875 51.90625 \nL 21.1875 42.671875 \nQ 23.828125 48.09375 27.96875 50.703125 \nQ 32.125 53.328125 38.09375 53.328125 \nQ 40.28125 53.328125 42.703125 52.984375 \nQ 45.125 52.640625 47.796875 52 \nz\n\" id=\"DejaVuSerif-114\"/>\n      <path d=\"M 21.578125 -9.515625 \nL 25 -0.875 \nL 5.609375 46.6875 \nL -0.296875 46.6875 \nL -0.296875 51.90625 \nL 23.578125 51.90625 \nL 23.578125 46.6875 \nL 15.28125 46.6875 \nL 29.890625 10.984375 \nL 44.484375 46.6875 \nL 36.71875 46.6875 \nL 36.71875 51.90625 \nL 56.203125 51.90625 \nL 56.203125 46.6875 \nL 50.390625 46.6875 \nL 26.609375 -11.71875 \nQ 24.171875 -17.78125 21.1875 -20 \nQ 18.21875 -22.21875 12.796875 -22.21875 \nQ 10.5 -22.21875 8.078125 -21.828125 \nQ 5.671875 -21.4375 3.21875 -20.703125 \nL 3.21875 -10.796875 \nL 7.8125 -10.796875 \nQ 8.109375 -14.109375 9.5 -15.546875 \nQ 10.890625 -17 13.8125 -17 \nQ 16.5 -17 18.140625 -15.5 \nQ 19.78125 -14.015625 21.578125 -9.515625 \nz\n\" id=\"DejaVuSerif-121\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.1475 132.947187)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSerif-97\"/>\n      <use x=\"59.619141\" xlink:href=\"#DejaVuSerif-99\"/>\n      <use x=\"115.625\" xlink:href=\"#DejaVuSerif-99\"/>\n      <use x=\"171.630859\" xlink:href=\"#DejaVuSerif-117\"/>\n      <use x=\"236.035156\" xlink:href=\"#DejaVuSerif-114\"/>\n      <use x=\"283.837891\" xlink:href=\"#DejaVuSerif-97\"/>\n      <use x=\"343.457031\" xlink:href=\"#DejaVuSerif-99\"/>\n      <use x=\"399.462891\" xlink:href=\"#DejaVuSerif-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p2079331ca8)\" d=\"M 65.542187 16.144545 \nL 74.73824 99.149927 \nL 83.934293 144.946 \nL 93.130345 149.239382 \nL 102.326398 152.101636 \nL 111.522451 173.568545 \nL 120.718503 172.137418 \nL 129.914556 177.861927 \nL 139.110609 187.879818 \nL 148.306661 183.586436 \nL 157.502714 180.724182 \nL 166.698766 185.017564 \nL 175.894819 186.448691 \nL 185.090872 193.604327 \nL 194.286924 190.742073 \nL 203.482977 190.742073 \nL 212.67903 189.310945 \nL 221.875082 190.742073 \nL 231.071135 195.035455 \nL 240.267187 193.604327 \n\" style=\"fill:none;stroke:#55a868;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.805937 203.98 \nL 56.805937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 249.003437 203.98 \nL 249.003437 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.805937 203.98 \nL 249.003437 203.98 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.805937 7.2 \nL 249.003437 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 306.088437 203.98 \nL 498.285937 203.98 \nL 498.285937 7.2 \nL 306.088437 7.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_6\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 314.824687 203.98 \nL 314.824687 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(306.077969 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 353.652465 203.98 \nL 353.652465 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(344.905747 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_16\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 392.480243 203.98 \nL 392.480243 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(383.733524 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 431.308021 203.98 \nL 431.308021 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0.6 -->\n      <g style=\"fill:#262626;\" transform=\"translate(422.561302 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_18\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 470.135799 203.98 \nL 470.135799 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0.8 -->\n      <g style=\"fill:#262626;\" transform=\"translate(461.38908 221.838281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- $\\epsilon$ -->\n     <g style=\"fill:#262626;\" transform=\"translate(398.887187 237.244062)scale(0.12 -0.12)\">\n      <use transform=\"translate(0 0.9375)\" xlink:href=\"#DejaVuSans-Oblique-949\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_8\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 306.088437 178.958638 \nL 498.285937 178.958638 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_21\">\n      <!-- 0.8 -->\n      <g style=\"fill:#262626;\" transform=\"translate(279.095 183.137778)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_20\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 306.088437 149.830525 \nL 498.285937 149.830525 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0.9 -->\n      <defs>\n       <path d=\"M 46.78125 32.671875 \nQ 43.5 29 39.25 27.1875 \nQ 35.015625 25.390625 29.6875 25.390625 \nQ 18.84375 25.390625 12.5625 31.9375 \nQ 6.296875 38.484375 6.296875 49.8125 \nQ 6.296875 60.890625 13.109375 67.546875 \nQ 19.921875 74.21875 31.296875 74.21875 \nQ 43.65625 74.21875 50.265625 65.015625 \nQ 56.890625 55.8125 56.890625 38.71875 \nQ 56.890625 19.578125 49.015625 9.078125 \nQ 41.15625 -1.421875 26.90625 -1.421875 \nQ 23.046875 -1.421875 18.796875 -0.6875 \nQ 14.546875 0.046875 10.109375 1.515625 \nL 10.109375 13.625 \nL 15.578125 13.625 \nQ 16.21875 8.6875 19.390625 6.046875 \nQ 22.5625 3.421875 27.875 3.421875 \nQ 37.359375 3.421875 41.984375 10.5625 \nQ 46.625 17.71875 46.78125 32.671875 \nz\nM 30.90625 69.390625 \nQ 23.96875 69.390625 20.28125 64.328125 \nQ 16.609375 59.28125 16.609375 49.8125 \nQ 16.609375 40.328125 20.28125 35.25 \nQ 23.96875 30.171875 30.90625 30.171875 \nQ 37.84375 30.171875 41.53125 35.078125 \nQ 45.21875 39.984375 45.21875 49.21875 \nQ 45.21875 58.9375 41.5 64.15625 \nQ 37.796875 69.390625 30.90625 69.390625 \nz\n\" id=\"DejaVuSerif-57\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(279.095 154.009666)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-57\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 306.088437 120.702412 \nL 498.285937 120.702412 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_23\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 14.203125 0 \nL 14.203125 5.171875 \nL 26.90625 5.171875 \nL 26.90625 65.828125 \nL 12.203125 56.296875 \nL 12.203125 62.703125 \nL 29.984375 74.21875 \nL 36.71875 74.21875 \nL 36.71875 5.171875 \nL 49.421875 5.171875 \nL 49.421875 0 \nz\n\" id=\"DejaVuSerif-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(279.095 124.881553)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_22\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 306.088437 91.574299 \nL 498.285937 91.574299 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_24\">\n      <!-- 1.1 -->\n      <g style=\"fill:#262626;\" transform=\"translate(279.095 95.75344)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 306.088437 62.446186 \nL 498.285937 62.446186 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_25\">\n      <!-- 1.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(279.095 66.625327)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_24\">\n      <path clip-path=\"url(#p471c2d66f4)\" d=\"M 306.088437 33.318074 \nL 498.285937 33.318074 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_26\">\n      <!-- 1.3 -->\n      <defs>\n       <path d=\"M 9.71875 69.828125 \nQ 15.4375 71.96875 20.671875 73.09375 \nQ 25.921875 74.21875 30.515625 74.21875 \nQ 41.21875 74.21875 47.21875 69.59375 \nQ 53.21875 64.984375 53.21875 56.78125 \nQ 53.21875 50.203125 49.0625 45.78125 \nQ 44.921875 41.359375 37.3125 39.796875 \nQ 46.296875 38.53125 51.25 33.28125 \nQ 56.203125 28.03125 56.203125 19.671875 \nQ 56.203125 9.46875 49.34375 4.015625 \nQ 42.484375 -1.421875 29.59375 -1.421875 \nQ 23.875 -1.421875 18.421875 -0.1875 \nQ 12.984375 1.03125 7.625 3.515625 \nL 7.625 17.671875 \nL 13.09375 17.671875 \nQ 13.578125 10.640625 17.828125 7.03125 \nQ 22.078125 3.421875 29.78125 3.421875 \nQ 37.25 3.421875 41.578125 7.734375 \nQ 45.90625 12.0625 45.90625 19.578125 \nQ 45.90625 28.171875 41.453125 32.59375 \nQ 37.015625 37.015625 28.421875 37.015625 \nL 23.78125 37.015625 \nL 23.78125 42 \nL 26.21875 42 \nQ 34.765625 42 39.03125 45.53125 \nQ 43.3125 49.078125 43.3125 56.203125 \nQ 43.3125 62.59375 39.796875 65.984375 \nQ 36.28125 69.390625 29.6875 69.390625 \nQ 23.09375 69.390625 19.453125 66.265625 \nQ 15.828125 63.140625 15.1875 56.984375 \nL 9.71875 56.984375 \nz\n\" id=\"DejaVuSerif-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(279.095 37.497214)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_27\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 20.515625 5.171875 \nL 29 5.171875 \nL 29 0 \nL 2.875 0 \nL 2.875 5.171875 \nL 11.53125 5.171875 \nL 11.53125 70.796875 \nL 2.875 70.796875 \nL 2.875 75.984375 \nL 20.515625 75.984375 \nz\n\" id=\"DejaVuSerif-108\"/>\n      <path d=\"M 30.078125 3.421875 \nQ 37.3125 3.421875 40.984375 9.125 \nQ 44.671875 14.84375 44.671875 25.984375 \nQ 44.671875 37.109375 40.984375 42.796875 \nQ 37.3125 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 19.171875 42.796875 \nQ 15.484375 37.109375 15.484375 25.984375 \nQ 15.484375 14.84375 19.1875 9.125 \nQ 22.90625 3.421875 30.078125 3.421875 \nz\nM 30.078125 -1.421875 \nQ 18.75 -1.421875 11.859375 6.078125 \nQ 4.984375 13.578125 4.984375 25.984375 \nQ 4.984375 38.375 11.84375 45.84375 \nQ 18.703125 53.328125 30.078125 53.328125 \nQ 41.453125 53.328125 48.3125 45.84375 \nQ 55.171875 38.375 55.171875 25.984375 \nQ 55.171875 13.578125 48.3125 6.078125 \nQ 41.453125 -1.421875 30.078125 -1.421875 \nz\n\" id=\"DejaVuSerif-111\"/>\n      <path d=\"M 5.609375 2.875 \nL 5.609375 14.984375 \nL 10.796875 14.984375 \nQ 10.984375 9.1875 14.421875 6.296875 \nQ 17.875 3.421875 24.609375 3.421875 \nQ 30.671875 3.421875 33.84375 5.6875 \nQ 37.015625 7.953125 37.015625 12.3125 \nQ 37.015625 15.71875 34.6875 17.8125 \nQ 32.375 19.921875 24.90625 22.3125 \nL 18.40625 24.515625 \nQ 11.71875 26.65625 8.71875 29.875 \nQ 5.71875 33.109375 5.71875 38.09375 \nQ 5.71875 45.21875 10.9375 49.265625 \nQ 16.15625 53.328125 25.390625 53.328125 \nQ 29.5 53.328125 34.03125 52.25 \nQ 38.578125 51.171875 43.40625 49.125 \nL 43.40625 37.796875 \nL 38.234375 37.796875 \nQ 38.03125 42.828125 34.703125 45.65625 \nQ 31.390625 48.484375 25.6875 48.484375 \nQ 20.015625 48.484375 17.109375 46.484375 \nQ 14.203125 44.484375 14.203125 40.484375 \nQ 14.203125 37.203125 16.40625 35.21875 \nQ 18.609375 33.25 25.203125 31.203125 \nL 32.328125 29 \nQ 39.703125 26.703125 42.9375 23.265625 \nQ 46.1875 19.828125 46.1875 14.40625 \nQ 46.1875 7.03125 40.546875 2.796875 \nQ 34.90625 -1.421875 25 -1.421875 \nQ 19.96875 -1.421875 15.1875 -0.34375 \nQ 10.40625 0.734375 5.609375 2.875 \nz\n\" id=\"DejaVuSerif-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(272.599375 117.27875)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSerif-108\"/>\n      <use x=\"31.982422\" xlink:href=\"#DejaVuSerif-111\"/>\n      <use x=\"92.1875\" xlink:href=\"#DejaVuSerif-115\"/>\n      <use x=\"143.505859\" xlink:href=\"#DejaVuSerif-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#p471c2d66f4)\" d=\"M 314.824687 195.035455 \nL 324.02074 63.592254 \nL 333.216793 42.149016 \nL 342.412845 34.147109 \nL 351.608898 28.623204 \nL 360.804951 26.243087 \nL 370.001003 24.0419 \nL 379.197056 22.524175 \nL 388.393109 20.738558 \nL 397.589161 19.621505 \nL 406.785214 19.633624 \nL 415.981266 18.921065 \nL 425.177319 18.41063 \nL 434.373372 17.703488 \nL 443.569424 17.168296 \nL 452.765477 17.230624 \nL 461.96153 16.794464 \nL 471.157582 16.598207 \nL 480.353635 16.468411 \nL 489.549687 16.144545 \n\" style=\"fill:none;stroke:#c44e52;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 306.088437 203.98 \nL 306.088437 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 498.285937 203.98 \nL 498.285937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 306.088437 203.98 \nL 498.285937 203.98 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 306.088437 7.2 \nL 498.285937 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2079331ca8\">\n   <rect height=\"196.78\" width=\"192.1975\" x=\"56.805937\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p471c2d66f4\">\n   <rect height=\"196.78\" width=\"192.1975\" x=\"306.088437\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB_g2v-cfMyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}